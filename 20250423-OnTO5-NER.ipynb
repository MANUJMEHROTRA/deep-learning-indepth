{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ec3b60ac-e5bc-4bc2-897b-a3319ccfca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from transformers import AutoModel,AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3594bdc3-51e2-48c6-8a0d-7b1d5e6afe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data/onto5/train00.json with 15000 records\n",
      "Processed data/onto5/train01.json with 15000 records\n",
      "Processed data/onto5/train02.json with 15000 records\n",
      "Processed data/onto5/train03.json with 14924 records\n",
      "Combined DataFrame with 59924 rows\n",
      "                                                tags  \\\n",
      "0                        [0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "1   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]   \n",
      "2                        [0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [People, start, their, own, businesses, for, m...  \n",
      "1  [But, a, chance, to, fill, out, sales, -, tax,...  \n",
      "2  [Red, tape, is, the, bugaboo, of, small, busin...  \n",
      "3  [Ironically, ,, the, person, who, wants, to, r...  \n",
      "4  [Yet, every, business, owner, has, to, face, t...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set the directory path\n",
    "data_dir = \"data/onto5/\"\n",
    "\n",
    "# Find all train*.json files in the specified directory\n",
    "json_files = glob.glob(os.path.join(data_dir, \"train*.json\"))\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Process each file\n",
    "for file_path in json_files:\n",
    "    # Initialize a list to store records from this file\n",
    "    records = []\n",
    "    \n",
    "    # Read the JSONL file\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                # Parse each line as a JSON object\n",
    "                record = json.loads(line)\n",
    "                records.append(record)\n",
    "    \n",
    "    # Convert records to a dataframe\n",
    "    if records:\n",
    "        df = pd.DataFrame(records)\n",
    "        dfs.append(df)\n",
    "        print(f\"Processed {file_path} with {len(records)} records\")\n",
    "\n",
    "# Concatenate all dataframes\n",
    "if dfs:\n",
    "    train_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Combined DataFrame with {len(train_df)} rows\")\n",
    "    print(train_df.head())\n",
    "else:\n",
    "    print(\"No data found in the JSON files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4fa5032e-9452-4b06-bf68-47319fd6c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(x):\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eeef490e-b6a0-4f4c-bf61-24754439c533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59924, 3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"sentence\"] = train_df[\"tokens\"].apply(get_sentence)\n",
    "train_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "323690a1-61cd-43bb-a6b4-e84ef663732f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[People, start, their, own, businesses, for, m...</td>\n",
       "      <td>People start their own businesses for many rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[But, a, chance, to, fill, out, sales, -, tax,...</td>\n",
       "      <td>But a chance to fill out sales - tax records i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[Red, tape, is, the, bugaboo, of, small, busin...</td>\n",
       "      <td>Red tape is the bugaboo of small business .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[Ironically, ,, the, person, who, wants, to, r...</td>\n",
       "      <td>Ironically , the person who wants to run his o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[Yet, every, business, owner, has, to, face, t...</td>\n",
       "      <td>Yet every business owner has to face the mound...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tags  \\\n",
       "0                        [0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]   \n",
       "2                        [0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [People, start, their, own, businesses, for, m...   \n",
       "1  [But, a, chance, to, fill, out, sales, -, tax,...   \n",
       "2  [Red, tape, is, the, bugaboo, of, small, busin...   \n",
       "3  [Ironically, ,, the, person, who, wants, to, r...   \n",
       "4  [Yet, every, business, owner, has, to, face, t...   \n",
       "\n",
       "                                            sentence  \n",
       "0  People start their own businesses for many rea...  \n",
       "1  But a chance to fill out sales - tax records i...  \n",
       "2        Red tape is the bugaboo of small business .  \n",
       "3  Ironically , the person who wants to run his o...  \n",
       "4  Yet every business owner has to face the mound...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "919e408f-79fa-421b-b6ab-7dc1b8606b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/onto5/label.json\",\"r\") as file:\n",
    "    tag_to_idx = json.load(file)\n",
    "    \n",
    "idx_to_idx = {j:i for i,j in tag_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "30d54b96-d7a4-4140-9e7e-ca31f43a13b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-CARDINAL': 1, 'B-DATE': 2, 'I-DATE': 3, 'B-PERSON': 4, 'I-PERSON': 5, 'B-NORP': 6, 'B-GPE': 7, 'I-GPE': 8, 'B-LAW': 9, 'I-LAW': 10, 'B-ORG': 11, 'I-ORG': 12, 'B-PERCENT': 13, 'I-PERCENT': 14, 'B-ORDINAL': 15, 'B-MONEY': 16, 'I-MONEY': 17, 'B-WORK_OF_ART': 18, 'I-WORK_OF_ART': 19, 'B-FAC': 20, 'B-TIME': 21, 'I-CARDINAL': 22, 'B-LOC': 23, 'B-QUANTITY': 24, 'I-QUANTITY': 25, 'I-NORP': 26, 'I-LOC': 27, 'B-PRODUCT': 28, 'I-TIME': 29, 'B-EVENT': 30, 'I-EVENT': 31, 'I-FAC': 32, 'B-LANGUAGE': 33, 'I-PRODUCT': 34, 'I-ORDINAL': 35, 'I-LANGUAGE': 36}\n",
      "{0: 'O', 1: 'B-CARDINAL', 2: 'B-DATE', 3: 'I-DATE', 4: 'B-PERSON', 5: 'I-PERSON', 6: 'B-NORP', 7: 'B-GPE', 8: 'I-GPE', 9: 'B-LAW', 10: 'I-LAW', 11: 'B-ORG', 12: 'I-ORG', 13: 'B-PERCENT', 14: 'I-PERCENT', 15: 'B-ORDINAL', 16: 'B-MONEY', 17: 'I-MONEY', 18: 'B-WORK_OF_ART', 19: 'I-WORK_OF_ART', 20: 'B-FAC', 21: 'B-TIME', 22: 'I-CARDINAL', 23: 'B-LOC', 24: 'B-QUANTITY', 25: 'I-QUANTITY', 26: 'I-NORP', 27: 'I-LOC', 28: 'B-PRODUCT', 29: 'I-TIME', 30: 'B-EVENT', 31: 'I-EVENT', 32: 'I-FAC', 33: 'B-LANGUAGE', 34: 'I-PRODUCT', 35: 'I-ORDINAL', 36: 'I-LANGUAGE'}\n"
     ]
    }
   ],
   "source": [
    "print(tag_to_idx)\n",
    "print(idx_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6dcab35c-71ff-41a0-8807-026cb224f8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSONL file\n",
    "\n",
    "test_data = []\n",
    "with open(\"data/onto5/test.json\", 'r') as file:\n",
    "    for line in file:\n",
    "        if line.strip():  # Skip empty lines\n",
    "            # Parse each line as a JSON object\n",
    "            record = json.loads(line)\n",
    "            test_data.append(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b0deddfb-3cbf-4a33-a401-1f7fe0baa335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 7, 0, 7, 0, ...</td>\n",
       "      <td>[The, following, were, among, Friday, 's, offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[11, 12, 12, 12]</td>\n",
       "      <td>[Dow, Chemical, Co., --]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[16, 17, 17, 0, 13, 14, 0, 0, 0, 2, 3, 3, 3, 0...</td>\n",
       "      <td>[$, 150, million, of, 8.55, %, senior, notes, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, ...</td>\n",
       "      <td>[The, issue, ,, which, is, puttable, back, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 11, 12, 12, 12, 12, 0, 0...</td>\n",
       "      <td>[Rated, single, -, A, -, 1, by, Moody, 's, Inv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tags  \\\n",
       "0  [0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 7, 0, 7, 0, ...   \n",
       "1                                   [11, 12, 12, 12]   \n",
       "2  [16, 17, 17, 0, 13, 14, 0, 0, 0, 2, 3, 3, 3, 0...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, ...   \n",
       "4  [0, 0, 0, 0, 0, 1, 0, 11, 12, 12, 12, 12, 0, 0...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [The, following, were, among, Friday, 's, offe...  \n",
       "1                           [Dow, Chemical, Co., --]  \n",
       "2  [$, 150, million, of, 8.55, %, senior, notes, ...  \n",
       "3  [The, issue, ,, which, is, puttable, back, to,...  \n",
       "4  [Rated, single, -, A, -, 1, by, Moody, 's, Inv...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(test_data)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c0880d8b-104f-425c-84e3-68d6aadd06f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8262, 3)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"sentence\"] = test_df[\"tokens\"].apply(get_sentence)\n",
    "test_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2953f116-dc16-4c84-91ca-0a9b18bdaee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8262"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "694d7992-9811-4598-8ffc-a2c4d7b675ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "test_df.head(2).to_clipboard()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "21d048fc-7e5c-4ea5-8cd8-34884c512e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2293, 2796, 102], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "tokenizer(\"I love Indian\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6dba33e8-404e-42e6-b0a4-1c2d61c82a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self,df,tokenizer,max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length=max_length\n",
    "        self.data = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        input_tokenize = self.tokenizer(row['sentence'],\n",
    "                                        truncation=True,\n",
    "                                        padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\")\n",
    "        input_ids = torch.tensor(input_tokenize['input_ids']).squeeze()\n",
    "        attention_mask = input_tokenize['attention_mask'].squeeze()\n",
    "        label = row[\"tags\"].copy()\n",
    "        label.insert(0, 100) \n",
    "        label.append(-100)\n",
    "        # Pad or truncate labels to match input_ids length\n",
    "        if len(label) < self.max_length:\n",
    "            label.extend([-100] * (self.max_length - len(label)))\n",
    "        elif len(label) > self.max_length:\n",
    "            label = label[:self.max_length]\n",
    "            \n",
    "        label = torch.tensor(label)\n",
    "        return input_ids,attention_mask,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bd41d074-28dd-4779-bfff-326bd455aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataSet = CustomDataSet(train_df,tokenizer,max_length=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "476be9c4-2f06-49b8-9c79-6a9edc223c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "300\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yq/ztb61z595ln533mzpd0ftylh0000gp/T/ipykernel_9295/169092292.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_tokenize['input_ids']).squeeze()\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataSet.__getitem__(1)[0]))\n",
    "print(len(train_dataSet.__getitem__(1)[1]))\n",
    "print(len(train_dataSet.__getitem__(1)[2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "41e0bd0f-5b56-4696-a2ac-3b316a1056db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 101, 2021, 1037, 3382, 2000, 6039, 2041, 4341, 1011, 4171, 2636, 2003,\n",
      "        6524, 2028, 1997, 2068, 1012,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([ 100,  100,  100,  100,  100,  100,  100,  100,  100,  100,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,    0,\n",
      "           0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yq/ztb61z595ln533mzpd0ftylh0000gp/T/ipykernel_9295/169092292.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_tokenize['input_ids']).squeeze()\n"
     ]
    }
   ],
   "source": [
    "print(train_dataSet.__getitem__(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3b25a4-e8a3-4ec9-8590-a1e07f8ba0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd52e8d-2450-4a3d-901f-95c293fbe291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
