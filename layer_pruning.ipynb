{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37fa968e-0498-4dba-bedd-469da3944418",
   "metadata": {},
   "source": [
    "#  Layer Pruning\n",
    "\n",
    "\n",
    "\n",
    "There are broadly two ways to prune:\n",
    "1. Remove layers = more robust. \n",
    "2. Reduce the width of the hidden dimension. This can be more effective, but only if you go beyond naive pruning where random weights are removed. This script only presents naive pruning and should be done with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaaba2c-8070-4e03-b1f7-ad0b6687f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip -q\n",
    "!pip install transformers -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dcadd5-b4d8-4e2f-94cb-e3ad9968cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the model\n",
    "# model_name = \"HuggingFaceTB/SmolLM-360M-instruct\"\n",
    "# model_name = \"HuggingFaceTB/SmolLM-135M-instruct\"\n",
    "\n",
    "# Base models\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Print the original model architecture\n",
    "print(\"Original model:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba1fe3-9608-4d02-af28-12565c8feb35",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e524065-4fc1-471a-b499-263c6fda4309",
   "metadata": {},
   "source": [
    "### Layer Pruning Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcddec9-90e9-4c36-ac26-4b09a8f9e6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Pruning\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def create_new_lm(model_name, target_params):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    print(\"Original model:\")\n",
    "    print(model)\n",
    "    \n",
    "    original_params = count_parameters(model)\n",
    "    print(f\"Original model parameters: {original_params:,}\")\n",
    "    \n",
    "    total_layers = len(model.model.layers)\n",
    "\n",
    "    # Calculate number of layers to keep\n",
    "    layers_to_keep = round((target_params / original_params) * total_layers)\n",
    "    layers_to_remove = total_layers - layers_to_keep\n",
    "\n",
    "    # Keep all layers except those right before the last layer\n",
    "    selected_layers = (\n",
    "        list(model.model.layers[:total_layers - layers_to_remove - 1]) + \n",
    "        [model.model.layers[-1]]\n",
    "    )\n",
    "    \n",
    "    model.model.layers = torch.nn.ModuleList(selected_layers)\n",
    "\n",
    "    model.config.num_hidden_layers = len(selected_layers)\n",
    "    \n",
    "    print(\"\\nModified model (AshokLM):\")\n",
    "    print(model)\n",
    "    \n",
    "    new_params = count_parameters(model)\n",
    "    print(f\"Modified model parameters: {new_params:,}\")\n",
    "    \n",
    "    reduction_percentage = (1 - new_params / original_params) * 100\n",
    "    print(f\"Size reduction: {reduction_percentage:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model_name = \"HuggingFaceTB/SmolLM-360M-Instruct\"\n",
    "# target_params = 200_000_000\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "target_params = 90_000_000\n",
    "\n",
    "new_lm = create_new_lm(model_name, target_params)\n",
    "\n",
    "modified_model_path = f\"{model_name.split('/')[1]}-layer-pruned-{int(target_params/1000000)}M-raw\"\n",
    "new_lm.save_pretrained(modified_model_path)\n",
    "print(f\"\\nAshokLM-100M-Instruct saved to: {modified_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6c5e02-6753-4a09-aac5-c653cefef92c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prune every second layer but not first or last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab2beb0-59ad-48cc-918b-fd3effb1c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def create_pruned_lm(model_name, target_params):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    print(\"Original model:\")\n",
    "    print(model)\n",
    "    \n",
    "    original_params = count_parameters(model)\n",
    "    print(f\"Original model parameters: {original_params:,}\")\n",
    "    \n",
    "    total_layers = len(model.model.layers)\n",
    "    \n",
    "    # Prune every second layer, keeping the first and last\n",
    "    selected_layers = [model.model.layers[i] for i in range(total_layers) if i == 0 or i == total_layers - 1 or i % 2 == 0]\n",
    "    \n",
    "    # Update the model layers and number of hidden layers\n",
    "    model.model.layers = torch.nn.ModuleList(selected_layers)\n",
    "    model.config.num_hidden_layers = len(selected_layers)\n",
    "    \n",
    "    print(\"\\nModified model (Pruned):\")\n",
    "    print(model)\n",
    "    \n",
    "    new_params = count_parameters(model)\n",
    "    print(f\"Modified model parameters: {new_params:,}\")\n",
    "    \n",
    "    reduction_percentage = (1 - new_params / original_params) * 100\n",
    "    print(f\"Size reduction: {reduction_percentage:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM-360M-Instruct\"\n",
    "target_params = 200_000_000\n",
    "\n",
    "pruned_lm = create_pruned_lm(model_name, target_params)\n",
    "\n",
    "modified_model_path = \"SmolLM-360M-Instruct-layer-pruned-every2nd-200M\"\n",
    "pruned_lm.save_pretrained(modified_model_path)\n",
    "print(f\"\\nPruned model saved to: {modified_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8eb355-423e-4475-b87d-294a20159184",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Layer + Width Pruning (naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb329cc7-764c-4fec-886a-f70759b4c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def prune_layers(model, target_params, original_params):\n",
    "    total_layers = len(model.model.layers)\n",
    "\n",
    "    # Calculate number of layers to keep\n",
    "    layers_to_keep = round((target_params / original_params) * total_layers)\n",
    "    layers_to_remove = total_layers - layers_to_keep\n",
    "\n",
    "    # Keep all layers except those right before the last layer\n",
    "    selected_layers = (\n",
    "        list(model.model.layers[:total_layers - layers_to_remove - 1]) + \n",
    "        [model.model.layers[-1]]\n",
    "    )\n",
    "    \n",
    "    # Assign pruned layers back to the model\n",
    "    model.model.layers = torch.nn.ModuleList(selected_layers)\n",
    "\n",
    "    # Update the config\n",
    "    model.config.num_hidden_layers = len(selected_layers)\n",
    "\n",
    "    return model\n",
    "\n",
    "def prune_hidden_dimensions(model, target_params, current_params):\n",
    "    original_hidden_size = model.config.hidden_size\n",
    "    original_intermediate_size = model.config.intermediate_size\n",
    "    original_proj_ratio = original_intermediate_size / original_hidden_size  # Calculate the ratio dynamically\n",
    "    num_heads = model.config.num_attention_heads\n",
    "\n",
    "    # Estimate new hidden size to target parameters\n",
    "    reduction_ratio = math.sqrt(target_params / current_params)\n",
    "    new_hidden_size = int(original_hidden_size * reduction_ratio)\n",
    "    new_hidden_size = (new_hidden_size // (2 * num_heads)) * (2 * num_heads)  # Ensure divisibility\n",
    "\n",
    "    num_attention_heads = model.config.num_attention_heads\n",
    "    num_key_value_heads = model.config.num_key_value_heads\n",
    "\n",
    "    # Update hidden size and intermediate size in the config\n",
    "    model.config.hidden_size = new_hidden_size\n",
    "    model.config.intermediate_size = int(new_hidden_size * original_proj_ratio)  # Maintain the original ratio\n",
    "\n",
    "    for layer in model.model.layers:\n",
    "        # Adjust attention projection layers\n",
    "        layer.self_attn.q_proj.weight = torch.nn.Parameter(\n",
    "            layer.self_attn.q_proj.weight[:new_hidden_size, :new_hidden_size].contiguous()\n",
    "        )\n",
    "        layer.self_attn.k_proj.weight = torch.nn.Parameter(\n",
    "            layer.self_attn.k_proj.weight[:new_hidden_size, :new_hidden_size // (num_attention_heads // num_key_value_heads)].contiguous()\n",
    "        )\n",
    "        layer.self_attn.v_proj.weight = torch.nn.Parameter(\n",
    "            layer.self_attn.v_proj.weight[:new_hidden_size, :new_hidden_size // (num_attention_heads // num_key_value_heads)].contiguous()\n",
    "        )\n",
    "        layer.self_attn.o_proj.weight = torch.nn.Parameter(\n",
    "            layer.self_attn.o_proj.weight[:new_hidden_size, :new_hidden_size].contiguous()\n",
    "        )\n",
    "\n",
    "        # Adjust MLP layers\n",
    "        new_intermediate_size = model.config.intermediate_size\n",
    "        layer.mlp.gate_proj.weight = torch.nn.Parameter(\n",
    "            layer.mlp.gate_proj.weight[:new_intermediate_size, :new_hidden_size].contiguous()\n",
    "        )\n",
    "        layer.mlp.up_proj.weight = torch.nn.Parameter(\n",
    "            layer.mlp.up_proj.weight[:new_intermediate_size, :new_hidden_size].contiguous()\n",
    "        )\n",
    "        layer.mlp.down_proj.weight = torch.nn.Parameter(\n",
    "            layer.mlp.down_proj.weight[:new_hidden_size, :new_intermediate_size].contiguous()\n",
    "        )\n",
    "\n",
    "    # Adjust rotary positional embeddings\n",
    "    rotary_dim = new_hidden_size // num_heads\n",
    "    model.model.rotary_emb.inv_freq = model.model.rotary_emb.inv_freq[:rotary_dim].contiguous()\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_new_lm(model_name, target_params_1, target_params_2):\n",
    "    # Step 1: Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    print(\"Original model:\")\n",
    "    print(model)\n",
    "    \n",
    "    # Count original parameters\n",
    "    original_params = count_parameters(model)\n",
    "    print(f\"Original model parameters: {original_params:,}\")\n",
    "    \n",
    "    # Step 2: Prune layers to target ~200M parameters\n",
    "    model = prune_layers(model, target_params_1, original_params)\n",
    "    \n",
    "    new_params = count_parameters(model)\n",
    "    print(f\"\\nModel parameters after layer pruning: {new_params:,}\")\n",
    "    \n",
    "    # Step 3: Prune hidden dimensions to target ~100M parameters\n",
    "    model = prune_hidden_dimensions(model, target_params_2, new_params)\n",
    "    \n",
    "    final_params = count_parameters(model)\n",
    "    print(f\"\\nModel parameters after hidden dimension pruning: {final_params:,}\")\n",
    "    \n",
    "    reduction_percentage = (1 - final_params / original_params) * 100\n",
    "    print(f\"\\nTotal size reduction: {reduction_percentage:.2f}%\")\n",
    "\n",
    "    print(model)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# # Model name and target parameters\n",
    "# model_name = \"HuggingFaceTB/SmolLM-360M-Instruct\"\n",
    "# target_params_1 = 250_000_000  # Target ~200M parameters after layer pruning\n",
    "# target_params_2 = 200_000_000  # Target ~100M parameters after hidden dimension pruning\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "target_params_1 = 110_000_000\n",
    "target_params_2 = 90_000_000\n",
    "\n",
    "# Create the pruned model\n",
    "new_lm = create_new_lm(model_name, target_params_1, target_params_2)\n",
    "\n",
    "modified_model_path = f\"{model_name.split('/')[1]}-layer-width-pruned-{int(target_params_2/1000000)}M-raw\"\n",
    "new_lm.save_pretrained(modified_model_path)\n",
    "print(f\"\\nAshokLM-100M-Instruct saved to: {modified_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb92ac84-da7e-418f-b3f6-52c74bb792a8",
   "metadata": {},
   "source": [
    "## Push to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea4cb20-8e17-458c-9c91-6f1e90273785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()\n",
    "## Enter your write token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55f2a1-3f41-48fd-8b8b-c49153442eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def push_model_to_hub(local_model_path, original_model_name, repo_name):\n",
    "    # Load the tokenizer from the original model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(original_model_name)\n",
    "    \n",
    "    # Load the modified model with ignore_mismatched_sizes\n",
    "    model = AutoModelForCausalLM.from_pretrained(local_model_path, \n",
    "                                                 torch_dtype=torch.bfloat16,\n",
    "                                                 ignore_mismatched_sizes=True # needed if pushing the model pruned by hidden dimension\n",
    "                                                )\n",
    "\n",
    "    # Push the model to the hub\n",
    "    model.push_to_hub(repo_name)\n",
    "    \n",
    "    # Push the tokenizer to the hub\n",
    "    tokenizer.push_to_hub(repo_name)\n",
    "\n",
    "    print(f\"Model and tokenizer pushed successfully to {repo_name}\")\n",
    "\n",
    "# Path to your local model is \"modified_model_path\"\n",
    "local_model_path = modified_model_path #to push the last model you pruned above.\n",
    "# local_model_path = \"SmolLM-360M-Instruct-layer-pruned-200M-raw\"\n",
    "\n",
    "# Set your repository name\n",
    "repository_name = f\"Ashok/{local_model_path}\"\n",
    "\n",
    "# Original model name\n",
    "# original_model_name = \"HuggingFaceTB/SmolLM-360M-Instruct\"\n",
    "original_model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "\n",
    "# Push the model\n",
    "push_model_to_hub(local_model_path, original_model_name, repository_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11492219-4cad-4aa9-bce2-d87fe2504b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
