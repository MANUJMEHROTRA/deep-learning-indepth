{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rH91txby6Oec",
    "outputId": "20a9ab43-94f5-4031-972a-f605af99d754"
   },
   "outputs": [],
   "source": [
    "# pip install onnxruntime fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03bhio4M5prw",
    "outputId": "8bb5f8c4-d30d-4991-929e-fdb418450ac9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft: 0.15.0\n",
      "Torch: 2.6.0\n",
      "Transformers: 4.50.0\n",
      "Accelerate: 1.5.2\n",
      "Huggingface Hub: 0.29.3\n"
     ]
    }
   ],
   "source": [
    "# --- Common Utilities and Setup ---\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "import accelerate\n",
    "import huggingface_hub\n",
    "import peft\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, accuracy_score\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, get_linear_schedule_with_warmup\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "import time\n",
    "import onnxruntime as ort\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import pickle\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "import warnings\n",
    "\n",
    "print(\"peft:\", peft.__version__)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n",
    "print(\"Huggingface Hub:\", huggingface_hub.__version__)\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "il_PA9ki5prx"
   },
   "outputs": [],
   "source": [
    "# Device Selection Function\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using Apple MPS GPU\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using NVIDIA CUDA GPU\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "VJqiDZ7d5pry"
   },
   "outputs": [],
   "source": [
    "def update_model_dict(model_alias, MODEL_NAME):\n",
    "    if not os.path.exists('model_dict.json'):\n",
    "        model_dict = {}\n",
    "    else:\n",
    "        with open('model_dict.json', 'r') as file:\n",
    "            model_dict = json.load(file)\n",
    "\n",
    "    model_dict[model_alias] = MODEL_NAME\n",
    "\n",
    "    with open('model_dict.json', 'w') as file:\n",
    "        json.dump(model_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "Dd7FZV0h5pry"
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(filepath=\"train-00000-of-00001-a5a7c6e4bb30b016.parquet\"):\n",
    "    \"\"\"Loads and preprocesses the dataset.\"\"\"\n",
    "    df = pd.read_parquet(filepath)\n",
    "    df = df[['conversation', 'issue_area']]\n",
    "    print(\"Original distribution:\\n\", df['issue_area'].value_counts())\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"labels\"] = label_encoder.fit_transform(df[\"issue_area\"])\n",
    "\n",
    "    #saving Label-encoder\n",
    "    label_encoder_path = f\"model-metric/{model_alias}/label_encoder.pkl\"\n",
    "    os.makedirs(os.path.dirname(label_encoder_path), exist_ok=True)\n",
    "    with open(label_encoder_path, \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "\n",
    "    return df, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "qbCVXPRG5pry"
   },
   "outputs": [],
   "source": [
    "def balance_dataset(df, max_count=100, random_state=42):\n",
    "    \"\"\"Balances the dataset using oversampling.\"\"\"\n",
    "    balanced_df = pd.DataFrame()\n",
    "    for issue in df['issue_area'].unique():\n",
    "        subset = df[df['issue_area'] == issue]\n",
    "        balanced_subset = resample(subset, replace=True, n_samples=max_count, random_state=random_state)\n",
    "        balanced_df = pd.concat([balanced_df, balanced_subset])\n",
    "    return balanced_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "db3b_IJo5pry"
   },
   "outputs": [],
   "source": [
    "def preprocess_conversation(conversation):\n",
    "    \"\"\"Preprocesses a conversation.\"\"\"\n",
    "    if isinstance(conversation, list):\n",
    "        return \" \".join([turn.get('text', '') for turn in conversation if isinstance(turn, dict)])\n",
    "    return str(conversation) #.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "nADn2AeU5pry"
   },
   "outputs": [],
   "source": [
    "class CustomT5Dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_source_length=300, max_target_length=10):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        conversation = row[\"conversation\"]\n",
    "        label = row[\"labels\"]\n",
    "\n",
    "        # Convert label to text representation\n",
    "        label_text = label_encoder.inverse_transform([label])[0]\n",
    "\n",
    "        # Prepare source (conversation) and target (intent) encodings\n",
    "        source_encoding = self.tokenizer(\n",
    "            conversation,\n",
    "            max_length=self.max_source_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        target_encoding = self.tokenizer(\n",
    "            label_text,\n",
    "            max_length=self.max_target_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": source_encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": source_encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": target_encoding[\"input_ids\"].squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "bc0Bln_P5pry"
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(df, tokenizer, batch_size=8, train_ratio=0.75):\n",
    "    \"\"\"Creates train and test DataLoaders.\"\"\"\n",
    "    train_size = int(train_ratio * len(df))\n",
    "    train_df, test_df = df[:train_size], df[train_size:]\n",
    "    train_dataset = CustomT5Dataset(train_df, tokenizer)\n",
    "    test_dataset = CustomT5Dataset(test_df, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "hAWCrKQD5prz"
   },
   "outputs": [],
   "source": [
    "class FlanT5WithLoRA(nn.Module):\n",
    "    def __init__(self, num_labels, lora_r=8, lora_alpha=16, lora_dropout=0.1):\n",
    "        super(FlanT5WithLoRA, self).__init__()\n",
    "        # Load the base T5 model\n",
    "        self.t5 = T5ForConditionalGeneration.from_pretrained(\n",
    "            \"google/flan-t5-base\"\n",
    "        )\n",
    "\n",
    "        # LoRA Configuration\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            # target_modules=[\"q\", \"k\", \"v\", \"o\"]\n",
    "            target_modules=[\"q\",  \"v\"]\n",
    "        )\n",
    "        self.t5 = get_peft_model(self.t5, lora_config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.t5(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "Fozd9oEc5prz"
   },
   "outputs": [],
   "source": [
    "# Function to compute class weights\n",
    "def compute_class_weights(labels, num_classes):\n",
    "    counter = Counter(labels)\n",
    "    total_samples = len(labels)\n",
    "    weights = [total_samples / (num_classes * counter[i]) for i in range(num_classes)]\n",
    "    return torch.tensor(weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "FyA9Weu15prz"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, tokenizer, model_alias, epochs=3, learning_rate=5e-5):\n",
    "    \"\"\"Trains the T5 model and saves logs, metrics, and model weights.\"\"\"\n",
    "\n",
    "    device = get_device()\n",
    "\n",
    "        # Additional MPS-specific configurations\n",
    "    if device.type == 'mps':\n",
    "        # Ensure float32 dtype for MPS compatibility\n",
    "        model = model.to(device, dtype=torch.float32)\n",
    "\n",
    "        # Some tensor operations may need explicit conversion\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "    else:\n",
    "        model = model.to(device)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Create directory for storing model metrics\n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # TensorBoard writer in the model directory\n",
    "    writer = SummaryWriter(log_dir=model_dir)\n",
    "\n",
    "    # Set up optimizer and learning rate scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    epoch_losses = []\n",
    "    metrics_data = []\n",
    "\n",
    "\n",
    "    # Predefined mapping if needed\n",
    "    label_mapping = {\n",
    "        label.lower(): label for label in label_encoder.classes_\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Predict intents by generating text\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.t5.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=128\n",
    "                )\n",
    "\n",
    "                # Decode generated texts\n",
    "                preds = [tokenizer.decode(g, skip_special_tokens=True).lower().strip() for g in generated_ids]\n",
    "                # true_labels = [tokenizer.decode(l, skip_special_tokens=True).lower().strip() for l in labels]\n",
    "                # true_labels = [tokenizer.decode(l.tolist(), skip_special_tokens=True).lower().strip() for l in labels]\n",
    "                true_labels = [tokenizer.decode(l, skip_special_tokens=True).split()[0].lower().strip() for l in labels.cpu().tolist()]\n",
    "\n",
    "\n",
    "\n",
    "                # Robust label matching\n",
    "                def match_label(pred):\n",
    "                    # Exact match\n",
    "                    if pred in label_mapping:\n",
    "                        return label_mapping[pred]\n",
    "\n",
    "                    # Partial match\n",
    "                    for label, mapped_label in label_mapping.items():\n",
    "                        if pred in label or label in pred:\n",
    "                            return mapped_label\n",
    "\n",
    "                    # Fallback to the first label if no match\n",
    "                    return label_encoder.classes_[0]\n",
    "\n",
    "                # Convert predictions and true labels\n",
    "                pred_indices = [label_encoder.transform([match_label(p)])[0] for p in preds]\n",
    "                # true_indices = labels.cpu().tolist()\n",
    "                # true_indices = [label_encoder.transform([match_label(tokenizer.decode(l[0], skip_special_tokens=True).lower().strip())])[0] for l in labels.cpu().tolist()]\n",
    "                true_indices = [label_encoder.transform([match_label(t)])[0] for t in true_labels]\n",
    "\n",
    "\n",
    "                all_preds.extend(pred_indices)\n",
    "                all_labels.extend(true_indices)\n",
    "\n",
    "            print(f\"all_preds type: {type(all_preds)}, shape: {len(all_preds)}, example: {all_preds[:5]}\")\n",
    "            print(f\"all_labels type: {type(all_labels)}, shape: {len(all_labels)}, example: {all_labels[:5]}\")\n",
    "\n",
    "            # Log batch loss every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                writer.add_scalar(\"BatchLoss/train\", loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "        # Rest of the function remains the same...\n",
    "\n",
    "        # Compute epoch metrics\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        # Store metrics for CSV logging\n",
    "        metrics_data.append([epoch + 1, avg_loss, accuracy, precision, recall, f1, epoch_time])\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Accuracy={accuracy:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1-score={f1:.4f}, Time={epoch_time:.2f}s\")\n",
    "\n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train\", accuracy, epoch)\n",
    "        writer.add_scalar(\"Precision/train\", precision, epoch)\n",
    "        writer.add_scalar(\"Recall/train\", recall, epoch)\n",
    "        writer.add_scalar(\"F1-score/train\", f1, epoch)\n",
    "        writer.add_scalar(\"Time/Epoch\", epoch_time, epoch)\n",
    "\n",
    "    # Save model KPIs as CSV\n",
    "    metrics_df = pd.DataFrame(metrics_data, columns=[\"Epoch\", \"Loss\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\", \"Time (s)\"])\n",
    "    metrics_df.to_csv(os.path.join(model_dir, \"training_metrics.csv\"), index=False)\n",
    "\n",
    "    # Save training loss curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, epochs + 1), epoch_losses, marker='o', linestyle='-', color='b')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    loss_plot_path = os.path.join(model_dir, \"training_loss.png\")\n",
    "    plt.savefig(loss_plot_path)\n",
    "    writer.add_figure(\"Training Loss\", plt.gcf(), close=True)\n",
    "\n",
    "    # Save model weights\n",
    "    model_path = os.path.join(model_dir, f\"{model_alias}.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "8m1rIcHt5prz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "def evaluate_model(model, test_loader, tokenizer, label_encoder, model_alias):\n",
    "    \"\"\"Evaluates the T5 model and saves metrics, logs, and confusion matrix.\"\"\"\n",
    "\n",
    "    device = get_device()\n",
    "\n",
    "        # Additional MPS-specific configurations\n",
    "    if device.type == 'mps':\n",
    "        model = model.to(device, dtype=torch.float32)\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "    else:\n",
    "        model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Predefined mapping\n",
    "    label_mapping = {\n",
    "        label.lower(): label for label in label_encoder.classes_\n",
    "    }\n",
    "\n",
    "    # Robust label matching function\n",
    "    def match_label(pred):\n",
    "        # Exact match\n",
    "        if pred in label_mapping:\n",
    "            return label_mapping[pred]\n",
    "\n",
    "        # Partial match\n",
    "        for label, mapped_label in label_mapping.items():\n",
    "            if pred in label or label in pred:\n",
    "                return mapped_label\n",
    "\n",
    "        # Fallback to the first label if no match\n",
    "        return label_encoder.classes_[0]\n",
    "\n",
    "    # Create directory for storing model metrics\n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter(log_dir=model_dir)\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Generate predictions\n",
    "            generated_ids = model.t5.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=128\n",
    "            )\n",
    "\n",
    "            # Decode generated texts\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True).lower().strip() for g in generated_ids]\n",
    "            true_labels = [tokenizer.decode(l, skip_special_tokens=True).lower().strip() for l in labels]\n",
    "\n",
    "            # Convert predicted and true intents to indices\n",
    "            pred_indices = [label_encoder.transform([match_label(p)])[0] for p in preds]\n",
    "            true_indices = [label_encoder.transform([match_label(t)])[0] for t in true_labels]\n",
    "\n",
    "            all_preds.extend(pred_indices)\n",
    "            all_labels.extend(true_indices)\n",
    "\n",
    "    eval_time = time.time() - start_time\n",
    "    class_names = label_encoder.classes_\n",
    "\n",
    "    # Compute metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
    "    class_metrics = pd.DataFrame({\n",
    "        'Class': class_names,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Support': support\n",
    "    })\n",
    "\n",
    "    overall_precision, overall_recall, overall_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    # Print and save classification report\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save confusion matrix plot\n",
    "    confusion_matrix_path = os.path.join(model_dir, \"confusion_matrix.png\")\n",
    "    plt.savefig(confusion_matrix_path)\n",
    "    writer.add_figure(\"Confusion Matrix\", plt.gcf(), close=True)\n",
    "\n",
    "    # Print overall metrics\n",
    "    print(\"\\nPer-class Metrics:\\n\", class_metrics.to_string(index=False))\n",
    "    print(f\"\\nOverall Metrics:\\nPrecision: {overall_precision:.4f}, Recall: {overall_recall:.4f}, F1-score: {overall_f1:.4f}, Eval Time: {eval_time:.2f}s\")\n",
    "\n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar(\"Precision/test\", overall_precision)\n",
    "    writer.add_scalar(\"Recall/test\", overall_recall)\n",
    "    writer.add_scalar(\"F1-score/test\", overall_f1)\n",
    "    writer.add_scalar(\"Evaluation Time\", eval_time)\n",
    "\n",
    "    # Log per-class metrics\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        writer.add_scalar(f\"Precision/{class_name}\", precision[i])\n",
    "        writer.add_scalar(f\"Recall/{class_name}\", recall[i])\n",
    "        writer.add_scalar(f\"F1-score/{class_name}\", f1[i])\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "    # Save evaluation metrics\n",
    "    class_metrics.to_csv(os.path.join(model_dir, \"class_metrics.csv\"), index=False)\n",
    "    cm_df.to_csv(os.path.join(model_dir, \"confusion_matrix.csv\"))\n",
    "\n",
    "    return class_metrics, cm_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "2U6Ka8dS5prz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def export_to_onnx(model, tokenizer, model_alias):\n",
    "    \"\"\"Exports the model to ONNX format.\"\"\"\n",
    "    model.eval().to(\"cpu\")\n",
    "    sample_input = tokenizer(\"test\", return_tensors=\"pt\")\n",
    "    input_names = [\"input_ids\", \"attention_mask\"]\n",
    "    output_names = [\"output\"]\n",
    "\n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    onnx_path = os.path.join(model_dir, f\"{model_alias}.onnx\")\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (sample_input[\"input_ids\"], sample_input[\"attention_mask\"]),\n",
    "        onnx_path,\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes={\n",
    "            \"input_ids\": {0: \"batch\", 1: \"sequence\"},\n",
    "            \"attention_mask\": {0: \"batch\", 1: \"sequence\"},\n",
    "            \"output\": {0: \"batch\"}\n",
    "        }\n",
    "    )\n",
    "    print(f\"ONNX model exported to {onnx_path}\")\n",
    "    return onnx_path\n",
    "\n",
    "def run_onnx_inference(onnx_path, input_ids, attention_mask):\n",
    "    \"\"\"Runs inference using ONNX Runtime.\"\"\"\n",
    "    ort_session = ort.InferenceSession(onnx_path)\n",
    "    ort_inputs = {\"input_ids\": input_ids.tolist(), \"attention_mask\": attention_mask.tolist()}\n",
    "    ort_outputs = ort_session.run(None, ort_inputs)\n",
    "    return torch.tensor(np.array(ort_outputs[0]), dtype=torch.float32)\n",
    "\n",
    "def compare_inference_performance(model, tokenizer, test_df, label_encoder, model_alias):\n",
    "    \"\"\"Compares inference performance between PyTorch and ONNX Runtime.\"\"\"\n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir=model_dir)\n",
    "\n",
    "    sample_batch = test_df.sample(50)\n",
    "    test_dataset_batch = CustomDataset(sample_batch, tokenizer)\n",
    "    test_loader_batch = DataLoader(test_dataset_batch, batch_size=len(sample_batch), shuffle=False)\n",
    "    batch = next(iter(test_loader_batch))\n",
    "    input_ids, attention_mask, labels = batch\n",
    "\n",
    "    # PyTorch Inference\n",
    "    model.eval().to(device)\n",
    "    start_time_torch = time.time()\n",
    "    with torch.no_grad():\n",
    "        torch_outputs = model(input_ids.to(device), attention_mask.to(device)).cpu()\n",
    "    latency_torch = time.time() - start_time_torch\n",
    "    throughput_torch = len(sample_batch) / latency_torch\n",
    "\n",
    "    # ONNX Inference\n",
    "    onnx_path = export_to_onnx(model, tokenizer, model_alias)\n",
    "    start_time_onnx = time.time()\n",
    "    onnx_outputs = run_onnx_inference(onnx_path, input_ids, attention_mask)\n",
    "    latency_onnx = time.time() - start_time_onnx\n",
    "    throughput_onnx = len(sample_batch) / latency_onnx\n",
    "\n",
    "    print(f\"PyTorch Inference - Latency: {latency_torch:.4f}s, Throughput: {throughput_torch:.2f} samples/s\")\n",
    "    print(f\"ONNX Inference - Latency: {latency_onnx:.4f}s, Throughput: {throughput_onnx:.2f} samples/s\")\n",
    "\n",
    "    torch_preds = torch.argmax(torch_outputs, dim=1).tolist()\n",
    "    onnx_preds = torch.argmax(onnx_outputs, dim=1).tolist()\n",
    "    actual_labels = labels.tolist()\n",
    "\n",
    "    # Compare predictions\n",
    "    torch_report = classification_report(actual_labels, torch_preds, target_names=label_encoder.classes_, output_dict=True)\n",
    "    onnx_report = classification_report(actual_labels, onnx_preds, target_names=label_encoder.classes_, output_dict=True)\n",
    "\n",
    "    torch_df = pd.DataFrame(torch_report).transpose()\n",
    "    onnx_df = pd.DataFrame(onnx_report).transpose()\n",
    "\n",
    "    torch_df.to_csv(os.path.join(model_dir, \"torch_classification_report.csv\"))\n",
    "    onnx_df.to_csv(os.path.join(model_dir, \"onnx_classification_report.csv\"))\n",
    "\n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar(\"Latency/PyTorch\", latency_torch)\n",
    "    writer.add_scalar(\"Throughput/PyTorch\", throughput_torch)\n",
    "    writer.add_scalar(\"Latency/ONNX\", latency_onnx)\n",
    "    writer.add_scalar(\"Throughput/ONNX\", throughput_onnx)\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "    return torch_df, onnx_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_class_weights(labels, num_classes):\n",
    "#     counter = Counter(labels)\n",
    "#     total_samples = len(labels)\n",
    "#     weights = [total_samples / (num_classes * counter[i]) for i in range(num_classes)]\n",
    "#     return torch.tensor(weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "nye8bnql5pr0"
   },
   "outputs": [],
   "source": [
    "# # Verify class weights\n",
    "# def compute_class_weights(labels, num_classes):\n",
    "#     class_counts = np.bincount(labels)\n",
    "#     total_samples = len(labels)\n",
    "#     weights = total_samples / (num_classes * class_counts)\n",
    "#     return torch.tensor(weights, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "ko5dWvSb5pr0"
   },
   "outputs": [],
   "source": [
    "# Main Training Script\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "model_alias = 'flan-t5-intent'\n",
    "update_model_dict(model_alias, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmkTln7C5pr0",
    "outputId": "88ceaa75-1710-44a6-86c3-40a061ccdf9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple MPS GPU\n",
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"device: {get_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K1EC6VMQ5pr0",
    "outputId": "1dcf817d-83fb-4864-e864-3193fe1ce1c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution:\n",
      " issue_area\n",
      "Cancellations and returns    286\n",
      "Order                        270\n",
      "Login and Account            151\n",
      "Shopping                     116\n",
      "Warranty                     105\n",
      "Shipping                      72\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df, label_encoder = load_and_preprocess_data()\n",
    "balanced_df = balance_dataset(df)\n",
    "balanced_df['conversation'] = balanced_df['conversation'].apply(preprocess_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "XipIOu-m5pr0"
   },
   "outputs": [],
   "source": [
    "# Tokenization and DataLoaders\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "luLx4_I65pr0"
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader, test_loader, test_df = create_dataloaders(balanced_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "pv5Qo5TH5pr0"
   },
   "outputs": [],
   "source": [
    "# Model Initialization and Training\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = FlanT5WithLoRA(num_labels=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution: Counter({0: 100, 5: 100, 3: 100, 1: 100, 2: 100, 4: 100})\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution\n",
    "class_distribution = Counter(balanced_df['labels'])\n",
    "print(\"Class Distribution:\", class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: tensor([1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weights(balanced_df['labels'], num_classes)\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch and nn: Core PyTorch library and neural network module.\n",
    "# autocast: For mixed-precision training (automatically scales floating-point operations to reduce memory usage and increase speed).\n",
    "# GradScaler: Scales the gradient values during mixed precision to prevent underflow.\n",
    "# time: For measuring latency and time metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache and sync before training\n",
    "torch.mps.empty_cache()\n",
    "torch.mps.synchronize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "Epoch 1/3\n",
      "Step 1/57 | Loss: 0.0346 | Accuracy: 0.9500 | Recall: 0.9500 | Precision: 0.9500 | F1 Score: 0.9485 | Perplexity: 1.627 | Latency: 13.318 sec\n",
      "Step 21/57 | Loss: 0.0459 | Accuracy: 0.9500 | Recall: 0.9500 | Precision: 0.9563 | F1 Score: 0.9508 | Perplexity: 1.802 | Latency: 1.551 sec\n",
      "Step 41/57 | Loss: 0.0096 | Accuracy: 1.0000 | Recall: 1.0000 | Precision: 1.0000 | F1 Score: 1.0000 | Perplexity: 1.274 | Latency: 1.211 sec\n",
      "Epoch Summary | Average Loss: 0.1143 | Accuracy: 0.9774 | Recall: 0.9774 | Precision: 0.9812 | F1 Score: 0.9770 | Perplexity: 1.6156 | Average Batch Latency: 2.3686 sec | Epoch Latency: 135.89sec\n",
      "\n",
      "Epoch 2/3\n",
      "Step 1/57 | Loss: 0.0294 | Accuracy: 0.9750 | Recall: 0.9750 | Precision: 0.9917 | F1 Score: 0.9750 | Perplexity: 1.295 | Latency: 0.778 sec\n",
      "Step 21/57 | Loss: 0.0452 | Accuracy: 0.9500 | Recall: 0.9500 | Precision: 0.9625 | F1 Score: 0.9542 | Perplexity: 1.199 | Latency: 0.828 sec\n",
      "Step 41/57 | Loss: 0.0207 | Accuracy: 0.9500 | Recall: 0.9500 | Precision: 0.9500 | F1 Score: 0.9500 | Perplexity: 1.411 | Latency: 1.168 sec\n",
      "Epoch Summary | Average Loss: 0.0762 | Accuracy: 0.9787 | Recall: 0.9787 | Precision: 0.9830 | F1 Score: 0.9785 | Perplexity: 1.3539 | Average Batch Latency: 1.7545 sec | Epoch Latency: 100.69sec\n",
      "\n",
      "Epoch 3/3\n",
      "Step 1/57 | Loss: 0.0105 | Accuracy: 0.9875 | Recall: 0.9875 | Precision: 0.9792 | F1 Score: 0.9825 | Perplexity: 1.200 | Latency: 0.829 sec\n",
      "Step 21/57 | Loss: 0.0056 | Accuracy: 1.0000 | Recall: 1.0000 | Precision: 1.0000 | F1 Score: 1.0000 | Perplexity: 1.155 | Latency: 1.387 sec\n",
      "Step 41/57 | Loss: 0.0308 | Accuracy: 0.9500 | Recall: 0.9500 | Precision: 0.9500 | F1 Score: 0.9500 | Perplexity: 1.188 | Latency: 3.725 sec\n",
      "Epoch Summary | Average Loss: 0.0555 | Accuracy: 0.9860 | Recall: 0.9860 | Precision: 0.9884 | F1 Score: 0.9856 | Perplexity: 1.1845 | Average Batch Latency: 2.0759 sec | Epoch Latency: 119.01sec\n",
      "\n",
      "Training Complete! ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Environment variables\n",
    "# The model uses Apple's Metal Performance Shaders (MPS) on your M4 Pro CPU if available, otherwise it falls back to CPU.\n",
    "# The environment variable PYTORCH_MPS_HIGH_WATERMARK_RATIO is set to 0.0, which disables the high watermark limit to avoid unnecessary \n",
    "# memory consumption, helping prevent OOM (Out of Memory) errors.\n",
    "\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
    "\n",
    "\n",
    "# Batch size: Set to 1 to avoid memory issues on your MPS device.\n",
    "# Gradient accumulation steps: Combines gradients over 4 batches before performing a backward pass to simulate a larger batch size, improving stability.\n",
    "# Epochs: The model will train for 5 iterations over the entire dataset.\n",
    "# Learning rate: Set to 1e-4 (a common value for fine-tuning models).\n",
    "# Max gradient norm: Uses gradient clipping at 1.0 to avoid exploding gradients.\n",
    "# Max length: Sequence length limited to 256 tokens.\n",
    "# Logits store: A list to collect model predictions for later analysis.\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 1  # Reduced batch size\n",
    "gradient_accumulation_steps = 4\n",
    "epochs = 3\n",
    "learning_rate = 1e-4\n",
    "max_norm = 1.0  # Gradient clipping\n",
    "max_length = 256  # Reduced sequence length\n",
    "logits_store = []\n",
    "\n",
    "# Custom forward method: Overrides the FlanT5WithLoRA model's forward pass to disable hidden states output, reducing memory usage.\n",
    "# This improves efficiency by only returning the logits and outputs, skipping the intermediate hidden states, which saves VRAM.\n",
    "\n",
    "# Disable decoder hidden states\n",
    "def forward_no_hidden_states(self, input_ids, attention_mask, labels):\n",
    "    outputs = self.t5(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels,\n",
    "        output_hidden_states=False  # No hidden states -> Less memory usage\n",
    "    )\n",
    "    logits = outputs.logits\n",
    "    return logits, outputs\n",
    "\n",
    "# Replace the original forward with the new one\n",
    "FlanT5WithLoRA.forward = forward_no_hidden_states\n",
    "\n",
    "\n",
    "# AdamW optimizer: Used for stable and efficient optimization with weight decay (0.01) to prevent overfitting.\n",
    "# Cross-entropy loss: Used as the loss function since the task is likely a classification problem.\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Ensures that all model parameters require gradient calculation, allowing them to be updated during backpropagation.\n",
    "\n",
    "# Ensure all parameters require gradients\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# GradScaler: Scales gradients to avoid numerical instability during mixed-precision training.\n",
    "\n",
    "# Mixed precision setup\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Calculates accuracy, precision, recall, F1-score, and perplexity for the model's predictions.\n",
    "# Perplexity: Measures how \"surprised\" the model is by the predictions (lower is better).\n",
    "# Avoids divide-by-zero errors using 1e-12 in the log calculation.\n",
    "\n",
    "# Metrics tracking\n",
    "def calculate_metrics(logits, labels):\n",
    "    \"\"\"Calculate accuracy, precision, recall, F1, and perplexity.\"\"\"\n",
    "    preds = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "    # Calculate perplexity (avoiding divide by zero)\n",
    "    logits = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    entropy = -np.sum(logits * np.log(logits + 1e-12), axis=1)\n",
    "    perplexity = np.exp(entropy).mean()\n",
    "\n",
    "    return accuracy, precision, recall, f1, perplexity\n",
    "\n",
    "# Iterates over the dataset for the defined number of epochs.\n",
    "# Tracks:\n",
    "# Running loss\n",
    "# Metrics: Accuracy, precision, recall, F1, and perplexity\n",
    "# Batch latencies\n",
    "\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    total_precision = 0.0\n",
    "    total_recall = 0.0\n",
    "    total_f1 = 0.0\n",
    "    total_perplexity = 0.0\n",
    "\n",
    "    batch_latencies = []\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ensure data is on the correct device\n",
    "        batch['input_ids'] = batch['input_ids'].to(device)\n",
    "        batch['attention_mask'] = batch['attention_mask'].to(device)\n",
    "        batch['labels'] = batch['labels'].to(device)\n",
    "\n",
    "        # Uses autocast() for mixed-precision training, reducing memory consumption and increasing performance.\n",
    "        # Accumulates gradients over multiple steps to simulate larger batch sizes.\n",
    "\n",
    "        \n",
    "        # Mixed precision with autocast\n",
    "        with autocast():\n",
    "            logits, outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            labels = batch[\"labels\"].view(-1)\n",
    "\n",
    "            loss = criterion(logits, labels) / gradient_accumulation_steps\n",
    "\n",
    "        # Backward pass with mixed precision\n",
    "        scaler.scale(loss).backward(retain_graph=True)\n",
    "\n",
    "        # Gradient accumulation: Only performs backward pass and optimizer step after multiple batches.\n",
    "        # Gradient clipping: Ensures stability by limiting gradients to max_norm (1.0).\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Calculate batch metrics\n",
    "        accuracy, precision, recall, f1, perplexity = calculate_metrics(logits, labels)\n",
    "\n",
    "        # Store sample logits\n",
    "        if step % 10 == 0:  # Store every 10 batches\n",
    "            logits_store.append(logits[:5].detach().cpu().numpy())\n",
    "\n",
    "        # Clear MPS cache\n",
    "        torch.mps.empty_cache()\n",
    "        torch.mps.synchronize()\n",
    "\n",
    "        # Batch latency\n",
    "        batch_latency = time.time() - batch_start_time\n",
    "        batch_latencies.append(batch_latency)\n",
    "\n",
    "        # Collects batch metrics and aggregates them for epoch-level evaluation.\n",
    "        # Aggregate metrics\n",
    "        running_loss += loss.item() * gradient_accumulation_steps\n",
    "        total_accuracy += accuracy\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "        total_perplexity += perplexity\n",
    "\n",
    "\n",
    "        # Print batch-level metrics\n",
    "        if step % 20 == 0:\n",
    "            print(f\"Step {step + 1}/{len(train_loader)} | Loss: {loss.item():.4f} | \"\n",
    "                  f\"Accuracy: {accuracy:.4f} | Recall: {recall:.4f} | Precision: {precision:.4f} | \"\n",
    "                  f\"F1 Score: {f1:.4f} | Perplexity: {perplexity:.3f} | Latency: {batch_latency:.3f} sec\")\n",
    "\n",
    "    # Epoch-level metrics\n",
    "    epoch_latency = time.time() - start_time\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    avg_accuracy = total_accuracy / len(train_loader)\n",
    "    avg_precision = total_precision / len(train_loader)\n",
    "    avg_recall = total_recall / len(train_loader)\n",
    "    avg_f1 = total_f1 / len(train_loader)\n",
    "    avg_perplexity = total_perplexity / len(train_loader)\n",
    "    avg_batch_latency = np.mean(batch_latencies)\n",
    "\n",
    "    print(f\"Epoch Summary | Average Loss: {avg_loss:.4f} | Accuracy: {avg_accuracy:.4f} | \"\n",
    "          f\"Recall: {avg_recall:.4f} | Precision: {avg_precision:.4f} | \"\n",
    "          f\"F1 Score: {avg_f1:.4f} | Perplexity: {avg_perplexity:.4f} | \"\n",
    "          f\"Average Batch Latency: {avg_batch_latency:.4f} sec | Epoch Latency: {epoch_latency:.2f}sec\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Store logits for further evaluation\n",
    "np.save(\"logits_store.npy\", np.array(logits_store))\n",
    "print(\"\\nTraining Complete! ðŸŽ‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nCtJIgm55pr0",
    "outputId": "67ccb67d-d15c-46cc-ba46-630ffb36e812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple MPS GPU\n",
      "\n",
      "Classification Report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "Cancellations and returns       0.63      0.95      0.76        20\n",
      "        Login and Account       1.00      0.96      0.98        28\n",
      "                    Order       0.95      0.60      0.73        30\n",
      "                 Shipping       0.85      0.96      0.90        23\n",
      "                 Shopping       0.95      0.91      0.93        22\n",
      "                 Warranty       1.00      1.00      1.00        27\n",
      "\n",
      "                 accuracy                           0.89       150\n",
      "                macro avg       0.90      0.90      0.88       150\n",
      "             weighted avg       0.91      0.89      0.89       150\n",
      "\n",
      "\n",
      "Per-class Metrics:\n",
      "                     Class  Precision   Recall  F1-Score  Support\n",
      "Cancellations and returns   0.633333 0.950000  0.760000       20\n",
      "        Login and Account   1.000000 0.964286  0.981818       28\n",
      "                    Order   0.947368 0.600000  0.734694       30\n",
      "                 Shipping   0.846154 0.956522  0.897959       23\n",
      "                 Shopping   0.952381 0.909091  0.930233       22\n",
      "                 Warranty   1.000000 1.000000  1.000000       27\n",
      "\n",
      "Overall Metrics:\n",
      "Precision: 0.9100, Recall: 0.8867, F1-score: 0.8857, Eval Time: 5.51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                       Class  Precision    Recall  F1-Score  Support\n",
       " 0  Cancellations and returns   0.633333  0.950000  0.760000       20\n",
       " 1          Login and Account   1.000000  0.964286  0.981818       28\n",
       " 2                      Order   0.947368  0.600000  0.734694       30\n",
       " 3                   Shipping   0.846154  0.956522  0.897959       23\n",
       " 4                   Shopping   0.952381  0.909091  0.930233       22\n",
       " 5                   Warranty   1.000000  1.000000  1.000000       27,\n",
       "                            Cancellations and returns  Login and Account  \\\n",
       " Cancellations and returns                         19                  0   \n",
       " Login and Account                                  0                 27   \n",
       " Order                                             10                  0   \n",
       " Shipping                                           1                  0   \n",
       " Shopping                                           0                  0   \n",
       " Warranty                                           0                  0   \n",
       " \n",
       "                            Order  Shipping  Shopping  Warranty  \n",
       " Cancellations and returns      1         0         0         0  \n",
       " Login and Account              0         0         1         0  \n",
       " Order                         18         2         0         0  \n",
       " Shipping                       0        22         0         0  \n",
       " Shopping                       0         2        20         0  \n",
       " Warranty                       0         0         0        27  )"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "evaluate_model(\n",
    "    model,\n",
    "    test_loader,\n",
    "    tokenizer,\n",
    "    label_encoder,\n",
    "    model_alias\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mIpxSDN75pr0",
    "outputId": "07928963-c7e2-4afe-f0d7-e4c86a06426c"
   },
   "outputs": [],
   "source": [
    "# train_model(model,train_loader, tokenizer,model_alias=model_alias, epochs=1, learning_rate=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5hKL5MM5pr0"
   },
   "outputs": [],
   "source": [
    "compare_inference_performance(model, tokenizer, test_df, label_encoder, model_alias=model_alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIm9L3FC5pr0"
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(f\"model-metric/{model_alias}/tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgsmygQZ5pr0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
