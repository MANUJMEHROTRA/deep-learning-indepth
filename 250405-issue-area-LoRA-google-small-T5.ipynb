{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft: 0.14.0\n",
      "Torch: 2.2.2\n",
      "Transformers: 4.49.0\n",
      "Accelerate: 1.4.0\n",
      "Huggingface Hub: 0.29.1\n"
     ]
    }
   ],
   "source": [
    "# --- Common Utilities and Setup ---\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "import accelerate\n",
    "import huggingface_hub\n",
    "import peft\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, accuracy_score\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, get_linear_schedule_with_warmup\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "import time\n",
    "import onnxruntime as ort\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import pickle\n",
    "\n",
    "print(\"peft:\", peft.__version__)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n",
    "print(\"Huggingface Hub:\", huggingface_hub.__version__)\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Selection Function\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using Apple MPS GPU\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using NVIDIA CUDA GPU\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_dict(model_alias, MODEL_NAME):\n",
    "    if not os.path.exists('model_dict.json'):\n",
    "        model_dict = {}\n",
    "    else:\n",
    "        with open('model_dict.json', 'r') as file:\n",
    "            model_dict = json.load(file)\n",
    "\n",
    "    model_dict[model_alias] = MODEL_NAME\n",
    "\n",
    "    with open('model_dict.json', 'w') as file:\n",
    "        json.dump(model_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(filepath=\"./data/train-00000-of-00001-a5a7c6e4bb30b016.parquet\"):\n",
    "    \"\"\"Loads and preprocesses the dataset.\"\"\"\n",
    "    df = pd.read_parquet(filepath)\n",
    "    df = df[['conversation', 'issue_area']]\n",
    "    print(\"Original distribution:\\n\", df['issue_area'].value_counts())\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"labels\"] = label_encoder.fit_transform(df[\"issue_area\"])\n",
    "\n",
    "    #saving Label-encoder\n",
    "    label_encoder_path = f\"model-metric/{model_alias}/label_encoder.pkl\"\n",
    "    os.makedirs(os.path.dirname(label_encoder_path), exist_ok=True)\n",
    "    with open(label_encoder_path, \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "        \n",
    "    return df, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(df, max_count=100, random_state=42):\n",
    "    \"\"\"Balances the dataset using oversampling.\"\"\"\n",
    "    balanced_df = pd.DataFrame()\n",
    "    for issue in df['issue_area'].unique():\n",
    "        subset = df[df['issue_area'] == issue]\n",
    "        balanced_subset = resample(subset, replace=True, n_samples=max_count, random_state=random_state)\n",
    "        balanced_df = pd.concat([balanced_df, balanced_subset])\n",
    "    return balanced_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_conversation(conversation):\n",
    "    \"\"\"Preprocesses a conversation.\"\"\"\n",
    "    if isinstance(conversation, list):\n",
    "        return \" \".join([turn.get('text', '') for turn in conversation if isinstance(turn, dict)])\n",
    "    return str(conversation) #.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PyTorch Dataset for T5\n",
    "class T5Dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512, prefix=\"classify: \"):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.prefix = prefix\n",
    "        self.label_names = dataframe['issue_area'].unique()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        \n",
    "        # Prepare input with prefix\n",
    "        text = self.prefix + str(row[\"conversation\"])\n",
    "        \n",
    "        # For T5, we need to prepare decoder input\n",
    "        target_text = str(row[\"issue_area\"])  # Use actual label text instead of numeric index\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize targets - using text_target parameter instead of as_target_tokenizer\n",
    "        labels = self.tokenizer(\n",
    "            text_target=target_text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=64,  # Shorter for labels\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(0)\n",
    "        labels = labels[\"input_ids\"].squeeze(0)\n",
    "        \n",
    "        # Replace tokenizer padding token id with -100 so it's ignored in loss calculation\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(df, tokenizer, batch_size=8, train_ratio=0.75):\n",
    "    \"\"\"Creates train and test DataLoaders.\"\"\"\n",
    "    train_size = int(train_ratio * len(df))\n",
    "    train_df, test_df = df[:train_size], df[train_size:]\n",
    "    \n",
    "    train_dataset = T5Dataset(train_df, tokenizer)\n",
    "    test_dataset = T5Dataset(test_df, tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5SmallWithLoRA(nn.Module):\n",
    "    def __init__(self, lora_r=8, lora_alpha=16, lora_dropout=0.1):\n",
    "        super(T5SmallWithLoRA, self).__init__()\n",
    "        # Load the base T5 model\n",
    "        self.t5 = T5ForConditionalGeneration.from_pretrained(\n",
    "            \"google-t5/t5-small\"\n",
    "        )\n",
    "        \n",
    "        # LoRA Configuration\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_2_SEQ_LM,  # For T5\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            target_modules=[\"q\", \"v\"]  # T5 attention modules to apply LoRA\n",
    "        )\n",
    "        self.t5 = get_peft_model(self.t5, lora_config)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.t5(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            labels=labels\n",
    "        )\n",
    "        # For classification tasks, you might need to process the output logits further\n",
    "        # but for training with T5, the loss is calculated correctly out of the box\n",
    "        return outputs\n",
    "        \n",
    "    # Add a prediction method for better control over generation\n",
    "    def predict(self, input_ids, attention_mask, tokenizer, max_length=32):\n",
    "        \"\"\"Generate text predictions with more control\"\"\"\n",
    "        generated_ids = self.t5.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            num_beams=4,  # Using more beams for better quality\n",
    "            length_penalty=0.6,  # Prefer shorter sequences\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        # Decode the generated IDs\n",
    "        predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        return predictions\n",
    "    def print_trainable_parameters(self):\n",
    "                \"\"\"\n",
    "                Prints the number of trainable parameters in the model.\n",
    "                \"\"\"\n",
    "                trainable_params = 0\n",
    "                all_param = 0\n",
    "                for _, param in self.named_parameters():\n",
    "                    all_param += param.numel()\n",
    "                    if param.requires_grad:\n",
    "                        trainable_params += param.numel()\n",
    "                print(\n",
    "                    f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
    "                )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "\n",
    "# # Load models\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "# # Create proper input format\n",
    "# # T5 expects input_ids and attention_mask\n",
    "# batch_size = 1\n",
    "# seq_length = 512\n",
    "\n",
    "# # Both model1 and model2 should be identical since they're loading the same model\n",
    "# # just using different import functions\n",
    "\n",
    "# # Create dummy input data within valid token range\n",
    "# input_ids = torch.randint(0, tokenizer.vocab_size, (batch_size, seq_length))\n",
    "# attention_mask = torch.ones(batch_size, seq_length, dtype=torch.long)\n",
    "\n",
    "# # For T5, we need decoder_input_ids for the complete picture\n",
    "# # Usually the first token is the pad token or bos token\n",
    "# decoder_input_ids = torch.ones(batch_size, 1, dtype=torch.long) * model.config.decoder_start_token_id\n",
    "\n",
    "# # Proper summary call with appropriate input shapes\n",
    "# summary(\n",
    "#     model,\n",
    "#     input_data=[input_ids, attention_mask, decoder_input_ids],\n",
    "#     depth=4,  # Reduce depth for cleaner output\n",
    "#     col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute class weights\n",
    "def compute_class_weights(labels, num_classes):\n",
    "    counter = Counter(labels)\n",
    "    total_samples = len(labels)\n",
    "    weights = [total_samples / (num_classes * counter[i]) for i in range(num_classes)]\n",
    "    return torch.tensor(weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def train_model(model, train_loader, model_alias, epochs=3, learning_rate=5e-5, tokenizer=None):\n",
    "    \"\"\"Trains the model and saves logs, metrics, and model weights with mixed precision training.\"\"\"\n",
    "    \n",
    "    # Suppress warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Device setup with proper fallback\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using Apple MPS GPU\")\n",
    "        os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'  # Prevent OOM errors\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using NVIDIA CUDA GPU\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Create directory for storing model metrics\n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # TensorBoard writer in the model directory\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter(log_dir=model_dir)\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    batch_size = 1  # Adjusted for memory constraints\n",
    "    gradient_accumulation_steps = 4\n",
    "    max_norm = 1.0  # Gradient clipping\n",
    "    \n",
    "    # Override forward method to save memory\n",
    "    def forward_no_hidden_states(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.t5(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=False  # Disable hidden states to save memory\n",
    "        )\n",
    "        return outputs.logits, outputs\n",
    "    \n",
    "    # Replace the original forward with memory-optimized version\n",
    "    original_forward = model.forward\n",
    "    model.forward = forward_no_hidden_states.__get__(model, type(model))\n",
    "    \n",
    "    # Set up optimizer with weight decay\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    # Mixed precision setup\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Make sure all parameters require gradients\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Metrics tracking function\n",
    "    def calculate_metrics(logits, labels):\n",
    "        \"\"\"Calculate accuracy, precision, recall, F1, and perplexity.\"\"\"\n",
    "        preds = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        \n",
    "        # Filter out padding tokens (-100)\n",
    "        valid_indices = labels != -100\n",
    "        if np.any(valid_indices):\n",
    "            labels = labels[valid_indices]\n",
    "            preds = preds[valid_indices]\n",
    "            \n",
    "            if len(labels) > 0:\n",
    "                accuracy = accuracy_score(labels, preds)\n",
    "                precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
    "                recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
    "                f1 = f1_score(labels, preds, average='weighted', zero_division=0)\n",
    "                \n",
    "                # Calculate perplexity (avoiding divide by zero)\n",
    "                logits_valid = logits[valid_indices].detach().cpu()\n",
    "                logits_softmax = torch.softmax(logits_valid, dim=-1).numpy()\n",
    "                entropy = -np.sum(logits_softmax * np.log(logits_softmax + 1e-12), axis=1)\n",
    "                perplexity = np.exp(entropy.mean())\n",
    "                \n",
    "                return accuracy, precision, recall, f1, perplexity\n",
    "        \n",
    "        # Return default values if no valid predictions\n",
    "        return 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    \n",
    "    epoch_losses = []\n",
    "    metrics_data = []\n",
    "    logits_store = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # For collecting data for metrics\n",
    "        total_accuracy = 0.0\n",
    "        total_precision = 0.0\n",
    "        total_recall = 0.0\n",
    "        total_f1 = 0.0\n",
    "        total_perplexity = 0.0\n",
    "        batch_latencies = []\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # Make sure data is on the correct device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Mixed precision with autocast\n",
    "            with autocast():\n",
    "                logits, outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss / gradient_accumulation_steps\n",
    "            \n",
    "            # Backward pass with mixed precision\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient accumulation to simulate larger batch size\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                # Gradient clipping for stability\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Calculate metrics for this batch\n",
    "            if logits.size(0) > 0:  # Ensure we have valid predictions\n",
    "                reshaped_logits = logits.view(-1, logits.size(-1))\n",
    "                reshaped_labels = labels.view(-1)\n",
    "                accuracy, precision, recall, f1, perplexity = calculate_metrics(reshaped_logits, reshaped_labels)\n",
    "                \n",
    "                # Aggregate metrics\n",
    "                total_accuracy += accuracy\n",
    "                total_precision += precision\n",
    "                total_recall += recall\n",
    "                total_f1 += f1\n",
    "                total_perplexity += perplexity\n",
    "            \n",
    "            # Store sample logits for analysis\n",
    "            if batch_idx % 10 == 0:\n",
    "                sample_logits = logits[:5].detach().cpu().numpy() if logits.size(0) >= 5 else logits.detach().cpu().numpy()\n",
    "                logits_store.append(sample_logits)\n",
    "            \n",
    "            # Clear cache for memory efficiency\n",
    "            if device.type == 'mps':\n",
    "                torch.mps.empty_cache()\n",
    "                torch.mps.synchronize()\n",
    "            elif device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Capture latency\n",
    "            batch_latency = time.time() - batch_start_time\n",
    "            batch_latencies.append(batch_latency)\n",
    "            running_loss += loss.item() * gradient_accumulation_steps\n",
    "            \n",
    "            # Log batch progress\n",
    "            if batch_idx % 20 == 0:\n",
    "                writer.add_scalar(\"BatchLoss/train\", loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "                writer.add_scalar(\"BatchPerplexity/train\", perplexity, epoch * len(train_loader) + batch_idx)\n",
    "                print(f\"Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f} | \"\n",
    "                      f\"Accuracy: {accuracy:.4f} | Recall: {recall:.4f} | Precision: {precision:.4f} | \"\n",
    "                      f\"F1: {f1:.4f} | Perplexity: {perplexity:.4f} | Latency: {batch_latency:.3f}s\")\n",
    "                \n",
    "                # Generate text predictions occasionally for monitoring\n",
    "                if batch_idx % 60 == 0 and tokenizer is not None:\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        # Generate predictions\n",
    "                        generated_ids = model.t5.generate(\n",
    "                            input_ids=input_ids[:1],  # Just use first example\n",
    "                            attention_mask=attention_mask[:1],\n",
    "                            max_length=8,  # Short for class labels\n",
    "                            num_beams=2,\n",
    "                            early_stopping=True\n",
    "                        )\n",
    "                        \n",
    "                        # Decode prediction\n",
    "                        prediction = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "                        \n",
    "                        # Get true label\n",
    "                        label_ids = labels[0].clone()\n",
    "                        label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "                        true_label = tokenizer.decode(label_ids, skip_special_tokens=True)\n",
    "                        \n",
    "                        print(f\"Example: Predicted '{prediction}' | True '{true_label}'\")\n",
    "                    model.train()\n",
    "        \n",
    "        # Epoch-level metrics\n",
    "        epoch_latency = time.time() - start_time\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        avg_accuracy = total_accuracy / len(train_loader)\n",
    "        avg_precision = total_precision / len(train_loader)\n",
    "        avg_recall = total_recall / len(train_loader)\n",
    "        avg_f1 = total_f1 / len(train_loader)\n",
    "        avg_perplexity = total_perplexity / len(train_loader)\n",
    "        avg_batch_latency = np.mean(batch_latencies)\n",
    "        \n",
    "        epoch_losses.append(avg_loss)\n",
    "        \n",
    "        # Store metrics for CSV logging\n",
    "        metrics_data.append([\n",
    "            epoch + 1, \n",
    "            avg_loss, \n",
    "            avg_perplexity,\n",
    "            avg_accuracy, \n",
    "            avg_precision, \n",
    "            avg_recall, \n",
    "            avg_f1,\n",
    "            epoch_latency\n",
    "        ])\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1} Summary:\")\n",
    "        print(f\"  Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Perplexity: {avg_perplexity:.4f}\")\n",
    "        print(f\"  Accuracy: {avg_accuracy:.4f}\")\n",
    "        print(f\"  Precision: {avg_precision:.4f}\")\n",
    "        print(f\"  Recall: {avg_recall:.4f}\")\n",
    "        print(f\"  F1 Score: {avg_f1:.4f}\")\n",
    "        print(f\"  Avg Batch Latency: {avg_batch_latency:.3f}s\")\n",
    "        print(f\"  Epoch Time: {epoch_latency:.2f}s\")\n",
    "        \n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "        writer.add_scalar(\"Perplexity/train\", avg_perplexity, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train\", avg_accuracy, epoch)\n",
    "        writer.add_scalar(\"Precision/train\", avg_precision, epoch)\n",
    "        writer.add_scalar(\"Recall/train\", avg_recall, epoch)\n",
    "        writer.add_scalar(\"F1/train\", avg_f1, epoch)\n",
    "        writer.add_scalar(\"Time/Epoch\", epoch_latency, epoch)\n",
    "    \n",
    "    # Save model KPIs as CSV\n",
    "    import pandas as pd\n",
    "    metrics_df = pd.DataFrame(metrics_data, columns=[\n",
    "        \"Epoch\", \"Loss\", \"Perplexity\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"Time (s)\"\n",
    "    ])\n",
    "    metrics_df.to_csv(os.path.join(model_dir, \"training_metrics.csv\"), index=False)\n",
    "    \n",
    "    # Save logits for further analysis\n",
    "    np.save(os.path.join(model_dir, \"logits_samples.npy\"), np.array(logits_store))\n",
    "    \n",
    "    # Save training loss curve\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, epochs + 1), epoch_losses, marker='o', linestyle='-', color='b')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    loss_plot_path = os.path.join(model_dir, \"training_loss.png\")\n",
    "    plt.savefig(loss_plot_path)\n",
    "    writer.add_figure(\"Training Loss\", plt.gcf(), close=True)\n",
    "    \n",
    "    # Save model weights\n",
    "    model_path = os.path.join(model_dir, f\"{model_alias}.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Save PEFT model - this retains LoRA adapters\n",
    "    peft_model_path = os.path.join(model_dir, \"peft_model\")\n",
    "    model.t5.save_pretrained(peft_model_path)\n",
    "    \n",
    "    # Restore original forward method\n",
    "    model.forward = original_forward\n",
    "    \n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    print(f\"Training complete! Model and metrics saved to {model_dir}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, tokenizer, label_encoder, model_alias):\n",
    "    \"\"\"\n",
    "    Evaluates the T5 model with improved memory handling and metrics calculation.\n",
    "    Saves metrics, logs, and confusion matrix to the specified model directory.\n",
    "    \n",
    "    Args:\n",
    "        model: The T5 model with LoRA adapters\n",
    "        test_loader: DataLoader for test data\n",
    "        tokenizer: The T5 tokenizer\n",
    "        label_encoder: Encoder to map between labels and indices\n",
    "        model_alias: Identifier for the model (used for saving)\n",
    "        \n",
    "    Returns:\n",
    "        class_metrics: DataFrame with per-class metrics\n",
    "        cm_df: DataFrame with confusion matrix\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import time\n",
    "    import torch\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Device setup with proper fallback\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using Apple MPS GPU for evaluation\")\n",
    "        os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'  # Prevent OOM errors\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using NVIDIA CUDA GPU for evaluation\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU for evaluation\")\n",
    "    \n",
    "    # Additional MPS-specific configurations\n",
    "    if device.type == 'mps':\n",
    "        model = model.to(device, dtype=torch.float32)\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "    else:\n",
    "        model = model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Create directory for storing model metrics\n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter(log_dir=model_dir)\n",
    "    \n",
    "    # Create a robust mapping function between predicted text and actual labels\n",
    "    # This helps in handling slightly different text predictions\n",
    "    def robust_label_matching(prediction, label_encoder):\n",
    "        \"\"\"Maps prediction text to closest label class\"\"\"\n",
    "        classes = label_encoder.classes_\n",
    "        prediction = prediction.lower().strip()\n",
    "        \n",
    "        # Direct match\n",
    "        for idx, label in enumerate(classes):\n",
    "            if prediction == label.lower():\n",
    "                return idx\n",
    "        \n",
    "        # Fuzzy matching - find closest label\n",
    "        closest_match = None\n",
    "        highest_similarity = 0\n",
    "        \n",
    "        for idx, label in enumerate(classes):\n",
    "            # Simple overlap check\n",
    "            if prediction in label.lower() or label.lower() in prediction:\n",
    "                similarity = len(set(prediction) & set(label.lower())) / max(len(prediction), len(label.lower()))\n",
    "                if similarity > highest_similarity:\n",
    "                    highest_similarity = similarity\n",
    "                    closest_match = idx\n",
    "        \n",
    "        if closest_match is not None and highest_similarity > 0.5:  # Threshold for similarity\n",
    "            return closest_match\n",
    "        \n",
    "        # If no good match found, return the first label (default)\n",
    "        return 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    all_generated_texts = []\n",
    "    all_true_texts = []\n",
    "    batch_latencies = []\n",
    "    generation_latencies = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            batch_start = time.time()\n",
    "            \n",
    "            # Move batch to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Generate predictions - time this separately\n",
    "            generation_start = time.time()\n",
    "            generated_ids = model.t5.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=64,  # Allow for longer generations\n",
    "                num_beams=4,    # Use beam search for better quality\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2  # Prevent repetition\n",
    "            )\n",
    "            generation_latency = time.time() - generation_start\n",
    "            generation_latencies.append(generation_latency)\n",
    "            \n",
    "            # Decode generated text predictions\n",
    "            generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            all_generated_texts.extend(generated_texts)\n",
    "            \n",
    "            # Get true label texts\n",
    "            # First replace -100 padding tokens with pad token ID\n",
    "            label_ids = labels.clone()\n",
    "            label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "            true_texts = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "            all_true_texts.extend(true_texts)\n",
    "            \n",
    "            # Convert to label indices\n",
    "            for pred_text, true_text in zip(generated_texts, true_texts):\n",
    "                pred_idx = robust_label_matching(pred_text, label_encoder)\n",
    "                \n",
    "                # Find true label index\n",
    "                try:\n",
    "                    true_idx = label_encoder.transform([true_text.strip()])[0]\n",
    "                except:\n",
    "                    # If true text not in encoder, find closest match\n",
    "                    true_idx = robust_label_matching(true_text, label_encoder)\n",
    "                \n",
    "                all_predictions.append(pred_idx)\n",
    "                all_true_labels.append(true_idx)\n",
    "            \n",
    "            # Clear memory cache\n",
    "            if device.type == 'mps':\n",
    "                torch.mps.empty_cache()\n",
    "                torch.mps.synchronize()\n",
    "            elif device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Track batch latency\n",
    "            batch_latency = time.time() - batch_start\n",
    "            batch_latencies.append(batch_latency)\n",
    "            \n",
    "            # Log progress\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Evaluating batch {batch_idx}/{len(test_loader)} - \"\n",
    "                      f\"Generation latency: {generation_latency:.3f}s, \"\n",
    "                      f\"Batch latency: {batch_latency:.3f}s\")\n",
    "                \n",
    "                # Print a few examples\n",
    "                if batch_idx % 20 == 0:\n",
    "                    for i in range(min(2, len(generated_texts))):\n",
    "                        print(f\"  Example {i}: Predicted '{generated_texts[i]}' | True '{true_texts[i]}'\")\n",
    "    \n",
    "    # Overall evaluation time\n",
    "    eval_time = time.time() - start_time\n",
    "    class_names = label_encoder.classes_\n",
    "    \n",
    "    # Calculate detailed metrics if we have valid predictions\n",
    "    if len(all_predictions) == 0 or len(all_true_labels) == 0:\n",
    "        print(\"No valid predictions found. Check model outputs and label mapping.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Analysis of prediction texts vs. true texts\n",
    "    prediction_analysis = defaultdict(int)\n",
    "    for pred, true in zip(all_generated_texts, all_true_texts):\n",
    "        key = f\"{true} â†’ {pred}\"\n",
    "        prediction_analysis[key] += 1\n",
    "    \n",
    "    # Get top 10 most common predictions \n",
    "    top_predictions = sorted(prediction_analysis.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"\\nTop 10 predictions:\")\n",
    "    for item, count in top_predictions:\n",
    "        print(f\"  {item}: {count}\")\n",
    "    \n",
    "    # Compute standard metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        all_true_labels, all_predictions, average=None, labels=range(len(class_names)), zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Create per-class metrics DataFrame\n",
    "    class_metrics = pd.DataFrame({\n",
    "        'Class': class_names,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Support': support\n",
    "    })\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_precision, overall_recall, overall_f1, _ = precision_recall_fscore_support(\n",
    "        all_true_labels, all_predictions, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    overall_accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "    \n",
    "    # Print classification report\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(\"\\nClassification Report:\\n\", \n",
    "          classification_report(all_true_labels, all_predictions, target_names=class_names, zero_division=0))\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_true_labels, all_predictions, labels=range(len(class_names)))\n",
    "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "    \n",
    "    # Calculate avg/max latencies\n",
    "    avg_batch_latency = np.mean(batch_latencies)\n",
    "    avg_generation_latency = np.mean(generation_latencies)\n",
    "    max_batch_latency = np.max(batch_latencies)\n",
    "    max_generation_latency = np.max(generation_latencies)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save confusion matrix plot\n",
    "    confusion_matrix_path = os.path.join(model_dir, \"confusion_matrix.png\")\n",
    "    plt.savefig(confusion_matrix_path)\n",
    "    writer.add_figure(\"Confusion Matrix\", plt.gcf(), close=True)\n",
    "    \n",
    "    # Plot metrics per class\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    class_metrics[['Class', 'Precision', 'Recall', 'F1-Score']].set_index('Class').plot(kind='bar')\n",
    "    plt.title('Performance Metrics by Class')\n",
    "    plt.ylabel('Score')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.tight_layout()\n",
    "    metrics_path = os.path.join(model_dir, \"class_metrics.png\")\n",
    "    plt.savefig(metrics_path)\n",
    "    writer.add_figure(\"Class Metrics\", plt.gcf(), close=True)\n",
    "    \n",
    "    # Print overall metrics\n",
    "    print(\"\\nPer-class Metrics Summary:\\n\", class_metrics.to_string(index=False))\n",
    "    print(f\"\\nOverall Metrics:\")\n",
    "    print(f\"  Accuracy: {overall_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {overall_precision:.4f}\")\n",
    "    print(f\"  Recall: {overall_recall:.4f}\")\n",
    "    print(f\"  F1-score: {overall_f1:.4f}\")\n",
    "    print(f\"  Eval Time: {eval_time:.2f}s\")\n",
    "    print(f\"  Avg Batch Latency: {avg_batch_latency:.4f}s\")\n",
    "    print(f\"  Avg Generation Latency: {avg_generation_latency:.4f}s\")\n",
    "    print(f\"  Max Batch Latency: {max_batch_latency:.4f}s\")\n",
    "    print(f\"  Max Generation Latency: {max_generation_latency:.4f}s\")\n",
    "    \n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar(\"Accuracy/test\", overall_accuracy)\n",
    "    writer.add_scalar(\"Precision/test\", overall_precision)\n",
    "    writer.add_scalar(\"Recall/test\", overall_recall)\n",
    "    writer.add_scalar(\"F1-score/test\", overall_f1)\n",
    "    writer.add_scalar(\"EvaluationTime/test\", eval_time)\n",
    "    writer.add_scalar(\"BatchLatency/test\", avg_batch_latency)\n",
    "    writer.add_scalar(\"GenerationLatency/test\", avg_generation_latency)\n",
    "    \n",
    "    # Log per-class metrics\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        writer.add_scalar(f\"Precision/{class_name}\", precision[i])\n",
    "        writer.add_scalar(f\"Recall/{class_name}\", recall[i])\n",
    "        writer.add_scalar(f\"F1-score/{class_name}\", f1[i])\n",
    "    \n",
    "    # Save text predictions for error analysis\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'True_Label': all_true_texts,\n",
    "        'Predicted_Text': all_generated_texts,\n",
    "        'True_Index': all_true_labels,\n",
    "        'Predicted_Index': all_predictions,\n",
    "        'Correct': [p == t for p, t in zip(all_predictions, all_true_labels)]\n",
    "    })\n",
    "    predictions_df.to_csv(os.path.join(model_dir, \"predictions.csv\"), index=False)\n",
    "    \n",
    "    # Save evaluation metrics\n",
    "    class_metrics.to_csv(os.path.join(model_dir, \"class_metrics.csv\"), index=False)\n",
    "    cm_df.to_csv(os.path.join(model_dir, \"confusion_matrix.csv\"))\n",
    "    \n",
    "    # Save latency metrics\n",
    "    latency_df = pd.DataFrame({\n",
    "        'Metric': ['Eval Time', 'Avg Batch Latency', 'Avg Generation Latency', \n",
    "                  'Max Batch Latency', 'Max Generation Latency'],\n",
    "        'Value': [eval_time, avg_batch_latency, avg_generation_latency, \n",
    "                 max_batch_latency, max_generation_latency]\n",
    "    })\n",
    "    latency_df.to_csv(os.path.join(model_dir, \"latency_metrics.csv\"), index=False)\n",
    "    \n",
    "    # Save overall metrics\n",
    "    overall_metrics_df = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "        'Value': [overall_accuracy, overall_precision, overall_recall, overall_f1]\n",
    "    })\n",
    "    overall_metrics_df.to_csv(os.path.join(model_dir, \"overall_metrics.csv\"), index=False)\n",
    "    \n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    print(f\"\\nEvaluation complete! Results saved to {model_dir}\")\n",
    "    return class_metrics, cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_onnx(model, tokenizer, model_alias):\n",
    "    \"\"\"Exports the model to ONNX format with proper handling for T5 models.\"\"\"\n",
    "    print(\"Starting ONNX export process...\")\n",
    "    \n",
    "    try:\n",
    "        # Move model to CPU for export\n",
    "        model = model.eval().to(\"cpu\")\n",
    "        \n",
    "        # Create sample input\n",
    "        prefix = \"classify: \"\n",
    "        text = prefix + \"This is a sample test input\"\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        sample_inputs = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Create model directory\n",
    "        model_dir = f\"model-metric/{model_alias}\"\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        onnx_path = os.path.join(model_dir, f\"{model_alias}.onnx\")\n",
    "        \n",
    "        # Create a custom wrapper class to handle T5 complexity\n",
    "        class T5GenerationWrapper(torch.nn.Module):\n",
    "            def __init__(self, t5_model, config):\n",
    "                super(T5GenerationWrapper, self).__init__()\n",
    "                self.model = t5_model\n",
    "                self.config = config\n",
    "                \n",
    "            def forward(self, input_ids, attention_mask):\n",
    "                # Create decoder_input_ids\n",
    "                batch_size = input_ids.shape[0]\n",
    "                decoder_input_ids = torch.ones(\n",
    "                    (batch_size, 1), \n",
    "                    dtype=torch.long\n",
    "                ) * self.config.decoder_start_token_id\n",
    "                \n",
    "                # For ONNX export, we need to do a forward pass, not generation\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=decoder_input_ids,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                \n",
    "                # Return logits tensor\n",
    "                return outputs.logits\n",
    "        \n",
    "        print(\"Creating T5 ONNX wrapper...\")\n",
    "        \n",
    "        # Get the T5 config\n",
    "        config = model.t5.config\n",
    "        \n",
    "        # Create the wrapper with the t5 model and its config\n",
    "        if hasattr(model, 't5'):\n",
    "            t5_model = model.t5\n",
    "        else:\n",
    "            t5_model = model  # If it's already the T5 model\n",
    "            \n",
    "        onnx_model = T5GenerationWrapper(t5_model, config)\n",
    "        \n",
    "        print(\"Beginning ONNX export...\")\n",
    "        \n",
    "        # Define input and output names\n",
    "        input_names = [\"input_ids\", \"attention_mask\"]\n",
    "        output_names = [\"logits\"]\n",
    "        \n",
    "        # Export to ONNX\n",
    "        torch.onnx.export(\n",
    "            onnx_model,\n",
    "            (sample_inputs[\"input_ids\"], sample_inputs[\"attention_mask\"]),\n",
    "            onnx_path,\n",
    "            input_names=input_names,\n",
    "            output_names=output_names,\n",
    "            dynamic_axes={\n",
    "                \"input_ids\": {0: \"batch\", 1: \"sequence\"},\n",
    "                \"attention_mask\": {0: \"batch\", 1: \"sequence\"},\n",
    "                \"logits\": {0: \"batch\", 1: \"sequence\", 2: \"vocab_size\"}\n",
    "            },\n",
    "            opset_version=15,  # Use higher opset version for better compatibility\n",
    "            do_constant_folding=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        print(f\"ONNX model successfully exported to {onnx_path}\")\n",
    "        return onnx_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ONNX export failed with error: {str(e)}\")\n",
    "        traceback_str = traceback.format_exc()\n",
    "        print(f\"Traceback: {traceback_str}\")\n",
    "        \n",
    "        # Create a minimal dummy ONNX model if export fails\n",
    "        try:\n",
    "            # Create a directory for the model\n",
    "            model_dir = f\"model-metric/{model_alias}\"\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            \n",
    "            # Path for the dummy model\n",
    "            dummy_onnx_path = os.path.join(model_dir, f\"{model_alias}_dummy.onnx\")\n",
    "            \n",
    "            print(f\"Creating minimal dummy ONNX model at {dummy_onnx_path}\")\n",
    "            \n",
    "            # Simple dummy model\n",
    "            class DummyModel(torch.nn.Module):\n",
    "                def forward(self, input_ids, attention_mask):\n",
    "                    # Simple identity operation\n",
    "                    return torch.zeros((input_ids.shape[0], input_ids.shape[1], 32128), dtype=torch.float32)\n",
    "            \n",
    "            dummy_model = DummyModel()\n",
    "            dummy_inputs = (\n",
    "                torch.ones((1, 10), dtype=torch.long),\n",
    "                torch.ones((1, 10), dtype=torch.long)\n",
    "            )\n",
    "            \n",
    "            torch.onnx.export(\n",
    "                dummy_model,\n",
    "                dummy_inputs,\n",
    "                dummy_onnx_path,\n",
    "                input_names=[\"input_ids\", \"attention_mask\"],\n",
    "                output_names=[\"logits\"],\n",
    "                dynamic_axes={\n",
    "                    \"input_ids\": {0: \"batch\", 1: \"sequence\"},\n",
    "                    \"attention_mask\": {0: \"batch\", 1: \"sequence\"},\n",
    "                    \"logits\": {0: \"batch\", 1: \"sequence\", 2: \"vocab_size\"}\n",
    "                },\n",
    "                opset_version=12\n",
    "            )\n",
    "            \n",
    "            print(f\"Created dummy ONNX model at {dummy_onnx_path}\")\n",
    "            return dummy_onnx_path\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to create dummy ONNX model: {str(e2)}\")\n",
    "            return None\n",
    "\n",
    "def run_onnx_inference(onnx_path, input_ids, attention_mask):\n",
    "    \"\"\"Runs inference using ONNX Runtime with proper error handling.\"\"\"\n",
    "    if onnx_path is None:\n",
    "        print(\"No valid ONNX model path provided\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        import traceback\n",
    "        import numpy as np\n",
    "        \n",
    "        print(f\"Starting ONNX inference with model: {onnx_path}\")\n",
    "        \n",
    "        # Verify the file exists\n",
    "        if not os.path.exists(onnx_path):\n",
    "            print(f\"ONNX model file not found at {onnx_path}\")\n",
    "            return None\n",
    "            \n",
    "        # Get available providers\n",
    "        available_providers = ort.get_available_providers()\n",
    "        print(f\"Available ONNX Runtime providers: {available_providers}\")\n",
    "        \n",
    "        # Select appropriate providers\n",
    "        selected_providers = []\n",
    "        if 'CUDAExecutionProvider' in available_providers:\n",
    "            selected_providers.append('CUDAExecutionProvider')\n",
    "        selected_providers.append('CPUExecutionProvider')\n",
    "        \n",
    "        print(f\"Using providers: {selected_providers}\")\n",
    "        \n",
    "        # Create ONNX Runtime session\n",
    "        try:\n",
    "            ort_session = ort.InferenceSession(onnx_path, providers=selected_providers)\n",
    "            print(\"Successfully created ONNX Runtime session\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating ONNX Runtime session with specified providers: {e}\")\n",
    "            print(\"Falling back to default provider configuration\")\n",
    "            ort_session = ort.InferenceSession(onnx_path)\n",
    "        \n",
    "        # Print input names expected by the model\n",
    "        input_names = [input.name for input in ort_session.get_inputs()]\n",
    "        output_names = [output.name for output in ort_session.get_outputs()]\n",
    "        print(f\"ONNX model expects inputs: {input_names}\")\n",
    "        print(f\"ONNX model provides outputs: {output_names}\")\n",
    "        \n",
    "        # Convert PyTorch tensors to NumPy arrays\n",
    "        input_ids_np = input_ids.cpu().numpy()\n",
    "        attention_mask_np = attention_mask.cpu().numpy()\n",
    "        \n",
    "        print(f\"Input shapes - input_ids: {input_ids_np.shape}, attention_mask: {attention_mask_np.shape}\")\n",
    "        \n",
    "        # Create input dictionary\n",
    "        ort_inputs = {\n",
    "            \"input_ids\": input_ids_np,\n",
    "            \"attention_mask\": attention_mask_np\n",
    "        }\n",
    "        \n",
    "        # Run inference\n",
    "        print(\"Running ONNX inference...\")\n",
    "        ort_outputs = ort_session.run(None, ort_inputs)\n",
    "        \n",
    "        # Check output\n",
    "        if not ort_outputs or len(ort_outputs) == 0:\n",
    "            print(\"ONNX Runtime returned empty outputs\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"ONNX output shape: {ort_outputs[0].shape}\")\n",
    "        \n",
    "        # Convert output back to PyTorch tensor\n",
    "        return torch.tensor(ort_outputs[0], dtype=torch.float32)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ONNX inference failed with error: {str(e)}\")\n",
    "        traceback_str = traceback.format_exc()\n",
    "        print(f\"Traceback: {traceback_str}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_inference_performance(model, tokenizer, test_df, label_encoder, model_alias):\n",
    "    \"\"\"Compares inference performance between PyTorch and ONNX Runtime.\"\"\"\n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir=model_dir)\n",
    "\n",
    "    # Ensure device is properly defined\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                        (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "    \n",
    "    # Create a small batch for testing\n",
    "    sample_batch = test_df.sample(min(50, len(test_df)))\n",
    "    test_dataset_batch = T5Dataset(sample_batch, tokenizer)\n",
    "    test_loader_batch = DataLoader(test_dataset_batch, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Get a single batch\n",
    "    batch = next(iter(test_loader_batch))\n",
    "    \n",
    "    # Properly extract tensors from the batch dictionary\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    \n",
    "    # Ensure tensors are the right shape\n",
    "    print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "    print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "    \n",
    "    # PyTorch Inference\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    start_time_torch = time.time()\n",
    "    with torch.no_grad():\n",
    "        # For T5 models, we need to create decoder inputs\n",
    "        # Create a decoder_input_ids tensor starting with the model's decoder_start_token_id\n",
    "        # Usually for T5 this is the pad token id\n",
    "        decoder_input_ids = torch.ones(\n",
    "            (input_ids.shape[0], 1), \n",
    "            dtype=torch.long, \n",
    "            device=device\n",
    "        ) * model.t5.config.decoder_start_token_id\n",
    "        \n",
    "        # Use generate() instead of forward() for inference with T5\n",
    "        generated_ids = model.t5.generate(\n",
    "            input_ids=input_ids.to(device),\n",
    "            attention_mask=attention_mask.to(device),\n",
    "            max_length=32,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        # Get predictions by decoding the generated ids\n",
    "        predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        print(f\"Generated predictions: {predictions}\")\n",
    "        \n",
    "        # For analysis purposes, try to get logits from a forward pass if needed\n",
    "        try:\n",
    "            # Try with decoder_input_ids explicitly specified\n",
    "            outputs = model.t5(\n",
    "                input_ids=input_ids.to(device),\n",
    "                attention_mask=attention_mask.to(device),\n",
    "                decoder_input_ids=decoder_input_ids\n",
    "            )\n",
    "            logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]\n",
    "            logits = logits.to('cpu')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not get logits: {e}\")\n",
    "            # Create dummy logits for timing purposes\n",
    "            logits = torch.zeros((input_ids.shape[0], 1, model.t5.config.vocab_size), device='cpu')\n",
    "        \n",
    "    latency_torch = time.time() - start_time_torch\n",
    "    throughput_torch = len(sample_batch) / latency_torch\n",
    "\n",
    "    # ONNX Inference - This part needs special handling due to T5 model complexity\n",
    "    try:\n",
    "        onnx_path = export_to_onnx(model, tokenizer, model_alias)\n",
    "        start_time_onnx = time.time()\n",
    "        onnx_outputs = run_onnx_inference(onnx_path, input_ids, attention_mask)\n",
    "        latency_onnx = time.time() - start_time_onnx\n",
    "        throughput_onnx = len(sample_batch) / latency_onnx\n",
    "        print(f\"ONNX Inference - Latency: {latency_onnx:.4f}s, Throughput: {throughput_onnx:.2f} samples/s\")\n",
    "    except Exception as e:\n",
    "        print(f\"ONNX export/inference failed: {e}\")\n",
    "        latency_onnx = float('inf')\n",
    "        throughput_onnx = 0\n",
    "        onnx_outputs = None\n",
    "\n",
    "    print(f\"PyTorch Inference - Latency: {latency_torch:.4f}s, Throughput: {throughput_torch:.2f} samples/s\")\n",
    "    \n",
    "    # Modified prediction handling to accommodate T5 model\n",
    "    if onnx_outputs is not None:\n",
    "        # For PyTorch predictions\n",
    "        if hasattr(model, 'predict'):\n",
    "            # Use custom predict method if available \n",
    "            torch_preds = model.predict(\n",
    "                input_ids=input_ids.to(device),\n",
    "                attention_mask=attention_mask.to(device),\n",
    "                tokenizer=tokenizer\n",
    "            )\n",
    "            torch_preds = [label_encoder.transform([pred.strip()])[0] for pred in torch_preds]\n",
    "        else:\n",
    "            # Fall back to argmax if predict not available\n",
    "            torch_preds = torch.argmax(logits, dim=-1).tolist()\n",
    "            \n",
    "        # For ONNX predictions (if successful)\n",
    "        onnx_preds = torch.argmax(onnx_outputs, dim=-1).tolist()\n",
    "        \n",
    "        # Getting actual labels requires decoding the T5 label IDs\n",
    "        label_ids = labels.clone()\n",
    "        label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "        actual_texts = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "        actual_labels = [label_encoder.transform([text.strip()])[0] for text in actual_texts]\n",
    "        \n",
    "        # Compare predictions if we have ONNX results\n",
    "        try:\n",
    "            torch_report = classification_report(actual_labels, torch_preds, \n",
    "                                               target_names=label_encoder.classes_, \n",
    "                                               output_dict=True)\n",
    "            onnx_report = classification_report(actual_labels, onnx_preds, \n",
    "                                              target_names=label_encoder.classes_, \n",
    "                                              output_dict=True)\n",
    "            \n",
    "            torch_df = pd.DataFrame(torch_report).transpose()\n",
    "            onnx_df = pd.DataFrame(onnx_report).transpose()\n",
    "            \n",
    "            torch_df.to_csv(os.path.join(model_dir, \"torch_classification_report.csv\"))\n",
    "            onnx_df.to_csv(os.path.join(model_dir, \"onnx_classification_report.csv\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating classification reports: {e}\")\n",
    "            torch_df = pd.DataFrame()\n",
    "            onnx_df = pd.DataFrame()\n",
    "    else:\n",
    "        # Only compute PyTorch metrics if ONNX failed\n",
    "        if hasattr(model, 'predict'):\n",
    "            torch_preds = model.predict(\n",
    "                input_ids=input_ids.to(device),\n",
    "                attention_mask=attention_mask.to(device),\n",
    "                tokenizer=tokenizer\n",
    "            )\n",
    "            torch_preds = [label_encoder.transform([pred.strip()])[0] for pred in torch_preds]\n",
    "        else:\n",
    "            torch_preds = torch.argmax(logits, dim=-1).tolist()\n",
    "            \n",
    "        label_ids = labels.clone()\n",
    "        label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "        actual_texts = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "        actual_labels = [label_encoder.transform([text.strip()])[0] for text in actual_texts]\n",
    "        \n",
    "        try:\n",
    "            torch_report = classification_report(actual_labels, torch_preds, \n",
    "                                               target_names=label_encoder.classes_, \n",
    "                                               output_dict=True)\n",
    "            torch_df = pd.DataFrame(torch_report).transpose()\n",
    "            torch_df.to_csv(os.path.join(model_dir, \"torch_classification_report.csv\"))\n",
    "            onnx_df = pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating PyTorch classification report: {e}\")\n",
    "            torch_df = pd.DataFrame()\n",
    "            onnx_df = pd.DataFrame()\n",
    "    \n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar(\"Latency/PyTorch\", latency_torch)\n",
    "    writer.add_scalar(\"Throughput/PyTorch\", throughput_torch)\n",
    "    \n",
    "    if onnx_outputs is not None:\n",
    "        writer.add_scalar(\"Latency/ONNX\", latency_onnx)\n",
    "        writer.add_scalar(\"Throughput/ONNX\", throughput_onnx)\n",
    "    \n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    return torch_df, onnx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(labels, num_classes):\n",
    "    counter = Counter(labels)\n",
    "    total_samples = len(labels)\n",
    "    weights = [total_samples / (num_classes * counter[i]) for i in range(num_classes)]\n",
    "    return torch.tensor(weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google-t5/t5-small\"\n",
    "model_alias = 't5-small-lora'\n",
    "update_model_dict(model_alias, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple MPS GPU\n",
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "print(f\"device: {get_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution:\n",
      " issue_area\n",
      "Cancellations and returns    286\n",
      "Order                        270\n",
      "Login and Account            151\n",
      "Shopping                     116\n",
      "Warranty                     105\n",
      "Shipping                      72\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df, label_encoder = load_and_preprocess_data()\n",
    "balanced_df = balance_dataset(df)\n",
    "balanced_df['conversation'] = balanced_df['conversation'].apply(preprocess_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['conversation', 'issue_area', 'labels'], dtype='object')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and DataLoaders\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader, test_loader, test_df = create_dataloaders(balanced_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization and Training\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = T5SmallWithLoRA()\n",
    "\n",
    "class_weights = compute_class_weights(balanced_df['labels'], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple MPS GPU\n",
      "\n",
      "Epoch 1/10\n",
      "Batch 0/57 | Loss: 2.1604 | Accuracy: 0.0294 | Recall: 0.0294 | Precision: 0.0588 | F1: 0.0392 | Perplexity: 15.3601 | Latency: 0.714s\n",
      "Example: Predicted '' | True 'Shopping'\n",
      "Batch 20/57 | Loss: 1.8366 | Accuracy: 0.0323 | Recall: 0.0323 | Precision: 0.0323 | F1: 0.0323 | Perplexity: 16.1005 | Latency: 0.408s\n",
      "Batch 40/57 | Loss: 1.7740 | Accuracy: 0.0000 | Recall: 0.0000 | Precision: 0.0000 | F1: 0.0000 | Perplexity: 32.5937 | Latency: 0.415s\n",
      "Epoch 1 Summary:\n",
      "  Loss: 7.7636\n",
      "  Perplexity: 27.9454\n",
      "  Accuracy: 0.0517\n",
      "  Precision: 0.0951\n",
      "  Recall: 0.0517\n",
      "  F1 Score: 0.0621\n",
      "  Avg Batch Latency: 0.422s\n",
      "  Epoch Time: 25.03s\n",
      "\n",
      "Epoch 2/10\n",
      "Batch 0/57 | Loss: 1.4565 | Accuracy: 0.0500 | Recall: 0.0500 | Precision: 0.0250 | F1: 0.0333 | Perplexity: 80.8076 | Latency: 0.462s\n",
      "Example: Predicted '' | True 'Shipping'\n",
      "Batch 20/57 | Loss: 1.1078 | Accuracy: 0.2500 | Recall: 0.2500 | Precision: 0.4500 | F1: 0.3167 | Perplexity: 33.3952 | Latency: 0.389s\n",
      "Batch 40/57 | Loss: 0.7362 | Accuracy: 0.4483 | Recall: 0.4483 | Precision: 0.4828 | F1: 0.4598 | Perplexity: 24.7386 | Latency: 0.400s\n",
      "Epoch 2 Summary:\n",
      "  Loss: 3.5740\n",
      "  Perplexity: 43.5544\n",
      "  Accuracy: 0.3260\n",
      "  Precision: 0.4521\n",
      "  Recall: 0.3260\n",
      "  F1 Score: 0.3607\n",
      "  Avg Batch Latency: 0.423s\n",
      "  Epoch Time: 25.09s\n",
      "\n",
      "Epoch 3/10\n",
      "Batch 0/57 | Loss: 0.4784 | Accuracy: 0.5185 | Recall: 0.5185 | Precision: 0.6296 | F1: 0.5556 | Perplexity: 13.9584 | Latency: 0.534s\n",
      "Example: Predicted '' | True 'Shopping'\n",
      "Batch 20/57 | Loss: 0.3769 | Accuracy: 0.8077 | Recall: 0.8077 | Precision: 0.8077 | F1: 0.8077 | Perplexity: 18.3944 | Latency: 0.387s\n",
      "Batch 40/57 | Loss: 0.4740 | Accuracy: 0.6522 | Recall: 0.6522 | Precision: 0.6522 | F1: 0.6522 | Perplexity: 6.3393 | Latency: 0.390s\n",
      "Epoch 3 Summary:\n",
      "  Loss: 1.6640\n",
      "  Perplexity: 11.5384\n",
      "  Accuracy: 0.6853\n",
      "  Precision: 0.7473\n",
      "  Recall: 0.6853\n",
      "  F1 Score: 0.6992\n",
      "  Avg Batch Latency: 0.416s\n",
      "  Epoch Time: 24.72s\n",
      "\n",
      "Epoch 4/10\n",
      "Batch 0/57 | Loss: 0.3310 | Accuracy: 0.8148 | Recall: 0.8148 | Precision: 0.7593 | F1: 0.7778 | Perplexity: 5.2597 | Latency: 0.673s\n",
      "Example: Predicted 'Order' | True 'Shipping'\n",
      "Batch 20/57 | Loss: 0.1814 | Accuracy: 0.8500 | Recall: 0.8500 | Precision: 0.8250 | F1: 0.8333 | Perplexity: 4.1940 | Latency: 0.393s\n",
      "Batch 40/57 | Loss: 0.2373 | Accuracy: 0.7826 | Recall: 0.7826 | Precision: 0.7536 | F1: 0.7652 | Perplexity: 2.9283 | Latency: 0.386s\n",
      "Epoch 4 Summary:\n",
      "  Loss: 0.8969\n",
      "  Perplexity: 4.4496\n",
      "  Accuracy: 0.8201\n",
      "  Precision: 0.8293\n",
      "  Recall: 0.8201\n",
      "  F1 Score: 0.8171\n",
      "  Avg Batch Latency: 0.433s\n",
      "  Epoch Time: 25.71s\n",
      "\n",
      "Epoch 5/10\n",
      "Batch 0/57 | Loss: 0.1894 | Accuracy: 0.8065 | Recall: 0.8065 | Precision: 0.8100 | F1: 0.8042 | Perplexity: 2.1647 | Latency: 0.394s\n",
      "Example: Predicted 'Order' | True 'Cancellations and returns'\n",
      "Batch 20/57 | Loss: 0.1122 | Accuracy: 0.8261 | Recall: 0.8261 | Precision: 0.8261 | F1: 0.8261 | Perplexity: 2.3702 | Latency: 0.395s\n",
      "Batch 40/57 | Loss: 0.0916 | Accuracy: 0.9333 | Recall: 0.9333 | Precision: 0.9704 | F1: 0.9421 | Perplexity: 2.1948 | Latency: 0.408s\n",
      "Epoch 5 Summary:\n",
      "  Loss: 0.5081\n",
      "  Perplexity: 2.4868\n",
      "  Accuracy: 0.8563\n",
      "  Precision: 0.8635\n",
      "  Recall: 0.8563\n",
      "  F1 Score: 0.8500\n",
      "  Avg Batch Latency: 0.427s\n",
      "  Epoch Time: 25.27s\n",
      "\n",
      "Epoch 6/10\n",
      "Batch 0/57 | Loss: 0.1525 | Accuracy: 0.7083 | Recall: 0.7083 | Precision: 0.6806 | F1: 0.6875 | Perplexity: 2.4904 | Latency: 0.602s\n",
      "Example: Predicted '' | True 'Cancellations and returns'\n",
      "Batch 20/57 | Loss: 0.0929 | Accuracy: 0.9091 | Recall: 0.9091 | Precision: 0.9242 | F1: 0.9061 | Perplexity: 1.5398 | Latency: 0.406s\n",
      "Batch 40/57 | Loss: 0.0663 | Accuracy: 0.8696 | Recall: 0.8696 | Precision: 0.8406 | F1: 0.8522 | Perplexity: 1.6687 | Latency: 0.370s\n",
      "Epoch 6 Summary:\n",
      "  Loss: 0.3691\n",
      "  Perplexity: 1.7466\n",
      "  Accuracy: 0.8717\n",
      "  Precision: 0.8884\n",
      "  Recall: 0.8717\n",
      "  F1 Score: 0.8705\n",
      "  Avg Batch Latency: 0.423s\n",
      "  Epoch Time: 25.13s\n",
      "\n",
      "Epoch 7/10\n",
      "Batch 0/57 | Loss: 0.1931 | Accuracy: 0.8636 | Recall: 0.8636 | Precision: 0.9470 | F1: 0.8682 | Perplexity: 1.5485 | Latency: 0.408s\n",
      "Example: Predicted '' | True 'Order'\n",
      "Batch 20/57 | Loss: 0.0943 | Accuracy: 0.9000 | Recall: 0.9000 | Precision: 0.9000 | F1: 0.8889 | Perplexity: 1.3335 | Latency: 0.366s\n",
      "Batch 40/57 | Loss: 0.0331 | Accuracy: 0.9545 | Recall: 0.9545 | Precision: 0.9545 | F1: 0.9545 | Perplexity: 1.4095 | Latency: 0.375s\n",
      "Epoch 7 Summary:\n",
      "  Loss: 0.2978\n",
      "  Perplexity: 1.5039\n",
      "  Accuracy: 0.9031\n",
      "  Precision: 0.9254\n",
      "  Recall: 0.9031\n",
      "  F1 Score: 0.9034\n",
      "  Avg Batch Latency: 0.396s\n",
      "  Epoch Time: 23.51s\n",
      "\n",
      "Epoch 8/10\n",
      "Batch 0/57 | Loss: 0.1218 | Accuracy: 0.7917 | Recall: 0.7917 | Precision: 0.8542 | F1: 0.7986 | Perplexity: 1.5176 | Latency: 0.378s\n",
      "Example: Predicted 'Shipping' | True 'Shopping'\n",
      "Batch 20/57 | Loss: 0.0520 | Accuracy: 0.9545 | Recall: 0.9545 | Precision: 1.0000 | F1: 0.9697 | Perplexity: 1.6719 | Latency: 0.373s\n",
      "Batch 40/57 | Loss: 0.0271 | Accuracy: 0.9583 | Recall: 0.9583 | Precision: 0.9722 | F1: 0.9556 | Perplexity: 1.3887 | Latency: 0.367s\n",
      "Epoch 8 Summary:\n",
      "  Loss: 0.2511\n",
      "  Perplexity: 1.3969\n",
      "  Accuracy: 0.9206\n",
      "  Precision: 0.9396\n",
      "  Recall: 0.9206\n",
      "  F1 Score: 0.9229\n",
      "  Avg Batch Latency: 0.400s\n",
      "  Epoch Time: 23.67s\n",
      "\n",
      "Epoch 9/10\n",
      "Batch 0/57 | Loss: 0.0454 | Accuracy: 0.9688 | Recall: 0.9688 | Precision: 1.0000 | F1: 0.9821 | Perplexity: 1.3022 | Latency: 0.579s\n",
      "Example: Predicted '' | True 'Order'\n",
      "Batch 20/57 | Loss: 0.0657 | Accuracy: 0.8667 | Recall: 0.8667 | Precision: 0.9000 | F1: 0.8778 | Perplexity: 1.3551 | Latency: 0.375s\n",
      "Batch 40/57 | Loss: 0.0300 | Accuracy: 1.0000 | Recall: 1.0000 | Precision: 1.0000 | F1: 1.0000 | Perplexity: 1.3553 | Latency: 0.370s\n",
      "Epoch 9 Summary:\n",
      "  Loss: 0.2333\n",
      "  Perplexity: 1.3299\n",
      "  Accuracy: 0.9246\n",
      "  Precision: 0.9414\n",
      "  Recall: 0.9246\n",
      "  F1 Score: 0.9258\n",
      "  Avg Batch Latency: 0.409s\n",
      "  Epoch Time: 24.35s\n",
      "\n",
      "Epoch 10/10\n",
      "Batch 0/57 | Loss: 0.0506 | Accuracy: 0.9259 | Recall: 0.9259 | Precision: 0.9444 | F1: 0.9259 | Perplexity: 1.3221 | Latency: 0.488s\n",
      "Example: Predicted 'Shopping' | True 'Order'\n",
      "Batch 20/57 | Loss: 0.0647 | Accuracy: 0.9630 | Recall: 0.9630 | Precision: 0.9383 | F1: 0.9481 | Perplexity: 1.2632 | Latency: 0.375s\n",
      "Batch 40/57 | Loss: 0.0421 | Accuracy: 0.9231 | Recall: 0.9231 | Precision: 0.9231 | F1: 0.9231 | Perplexity: 1.3010 | Latency: 0.374s\n",
      "Epoch 10 Summary:\n",
      "  Loss: 0.2002\n",
      "  Perplexity: 1.2755\n",
      "  Accuracy: 0.9367\n",
      "  Precision: 0.9506\n",
      "  Recall: 0.9367\n",
      "  F1 Score: 0.9367\n",
      "  Avg Batch Latency: 0.410s\n",
      "  Epoch Time: 24.30s\n",
      "Training complete! Model and metrics saved to model-metric/t5-small-lora\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    model_alias=model_alias,\n",
    "    epochs=10,\n",
    "    learning_rate=5e-5,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Using Apple MPS GPU for evaluation\n",
      "Evaluating batch 0/19 - Generation latency: 3.323s, Batch latency: 3.332s\n",
      "  Example 0: Predicted 'Wendendtrutrues towstwiss-truth-to-read-in-re-ordering-where-saved-wift-all-Ge-distribution-me-first-ever-detail-' | True 'Login and Account'\n",
      "  Example 1: Predicted 'Login and Account' | True 'Login and Account'\n",
      "Evaluating batch 10/19 - Generation latency: 2.827s, Batch latency: 2.831s\n",
      "\n",
      "Top 10 predictions:\n",
      "  Login and Account â†’ Login and Account: 21\n",
      "  Warranty â†’ Warranty: 17\n",
      "  Shipping â†’ Shipping: 16\n",
      "  Order â†’ Order: 13\n",
      "  Shopping â†’ Shopping: 13\n",
      "  Cancellations and returns â†’ Cancellations and returns: 10\n",
      "  Order â†’ Shopping: 6\n",
      "  Cancellations and returns â†’ Login and Account: 3\n",
      "  Shopping â†’ Shopping Shopping: 3\n",
      "  Cancellations and returns â†’ Order: 3\n",
      "\n",
      "Classification Report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "Cancellations and returns       0.26      0.70      0.38        20\n",
      "        Login and Account       0.88      0.75      0.81        28\n",
      "                    Order       0.72      0.43      0.54        30\n",
      "                 Shipping       0.89      0.70      0.78        23\n",
      "                 Shopping       0.65      0.59      0.62        22\n",
      "                 Warranty       1.00      0.63      0.77        27\n",
      "\n",
      "                 accuracy                           0.63       150\n",
      "                macro avg       0.73      0.63      0.65       150\n",
      "             weighted avg       0.75      0.63      0.66       150\n",
      "\n",
      "\n",
      "Per-class Metrics Summary:\n",
      "                     Class  Precision   Recall  F1-Score  Support\n",
      "Cancellations and returns   0.264151 0.700000  0.383562       20\n",
      "        Login and Account   0.875000 0.750000  0.807692       28\n",
      "                    Order   0.722222 0.433333  0.541667       30\n",
      "                 Shipping   0.888889 0.695652  0.780488       23\n",
      "                 Shopping   0.650000 0.590909  0.619048       22\n",
      "                 Warranty   1.000000 0.629630  0.772727       27\n",
      "\n",
      "Overall Metrics:\n",
      "  Accuracy: 0.6267\n",
      "  Precision: 0.7546\n",
      "  Recall: 0.6267\n",
      "  F1-score: 0.6598\n",
      "  Eval Time: 30.32s\n",
      "  Avg Batch Latency: 1.5845s\n",
      "  Avg Generation Latency: 1.5794s\n",
      "  Max Batch Latency: 3.3321s\n",
      "  Max Generation Latency: 3.3226s\n",
      "\n",
      "Evaluation complete! Results saved to model-metric/t5-small-lora\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # Evaluate model\n",
    "print(\"Evaluating model...\")\n",
    "class_metrics, cm = evaluate_model(\n",
    "    trained_model, \n",
    "    test_loader, \n",
    "    tokenizer, \n",
    "    label_encoder, \n",
    "    model_alias\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([1, 512])\n",
      "Attention mask shape: torch.Size([1, 512])\n",
      "Generated predictions: ['']\n",
      "Starting ONNX export process...\n",
      "Creating T5 ONNX wrapper...\n",
      "Beginning ONNX export...\n",
      "ONNX model successfully exported to model-metric/t5-small-lora/t5-small-lora.onnx\n",
      "Starting ONNX inference with model: model-metric/t5-small-lora/t5-small-lora.onnx\n",
      "Available ONNX Runtime providers: ['CoreMLExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "Using providers: ['CPUExecutionProvider']\n",
      "Successfully created ONNX Runtime session\n",
      "ONNX model expects inputs: ['input_ids', 'attention_mask']\n",
      "ONNX model provides outputs: ['logits']\n",
      "Input shapes - input_ids: (1, 512), attention_mask: (1, 512)\n",
      "Running ONNX inference...\n",
      "ONNX output shape: (1, 1, 32128)\n",
      "ONNX Inference - Latency: 0.4167s, Throughput: 119.99 samples/s\n",
      "PyTorch Inference - Latency: 3.3246s, Throughput: 15.04 samples/s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompare_inference_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_alias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_alias\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[118], line 95\u001b[0m, in \u001b[0;36mcompare_inference_performance\u001b[0;34m(model, tokenizer, test_df, label_encoder, model_alias)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m onnx_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# For PyTorch predictions\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;66;03m# Use custom predict method if available \u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m         torch_preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m         torch_preds \u001b[38;5;241m=\u001b[39m [label_encoder\u001b[38;5;241m.\u001b[39mtransform([pred\u001b[38;5;241m.\u001b[39mstrip()])[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m torch_preds]\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;66;03m# Fall back to argmax if predict not available\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[79], line 32\u001b[0m, in \u001b[0;36mT5SmallWithLoRA.predict\u001b[0;34m(self, input_ids, attention_mask, tokenizer, max_length)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, tokenizer, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text predictions with more control\"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Using more beams for better quality\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Prefer shorter sequences\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Decode the generated IDs\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/repository/lora-training-explaination/verizon-poc/.venv/lib/python3.12/site-packages/peft/peft_model.py:2109\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   2107\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2108\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 2109\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/Desktop/repository/lora-training-explaination/verizon-poc/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/repository/lora-training-explaination/verizon-poc/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2038\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2034\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`attention_mask` passed to `generate` must be 2D.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   2037\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 2038\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2039\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m   2040\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2042\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m~/Desktop/repository/lora-training-explaination/verizon-poc/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:671\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    669\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    670\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 671\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m~/Desktop/repository/lora-training-explaination/verizon-poc/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/repository/lora-training-explaination/verizon-poc/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/repository/lora-training-explaination/verizon-poc/.venv/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1002\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to initialize the model with valid token embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1002\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1004\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/repository/lora-training-explaination/verizon-poc/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/repository/lora-training-explaination/verizon-poc/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/repository/lora-training-explaination/verizon-poc/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/repository/lora-training-explaination/verizon-poc/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "compare_inference_performance(trained_model, tokenizer, test_df, label_encoder, model_alias=model_alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenizer to model-metric/t5-small-lora/tokenizer/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('model-metric/t5-small-lora/tokenizer/tokenizer_config.json',\n",
       " 'model-metric/t5-small-lora/tokenizer/special_tokens_map.json',\n",
       " 'model-metric/t5-small-lora/tokenizer/spiece.model',\n",
       " 'model-metric/t5-small-lora/tokenizer/added_tokens.json',\n",
       " 'model-metric/t5-small-lora/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save tokenizer\n",
    "tokenizer_path = f\"model-metric/{model_alias}/tokenizer/\"\n",
    "print(f\"Saving tokenizer to {tokenizer_path}\")\n",
    "tokenizer.save_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluation completed. Model and metrics saved in model-metric/t5-small-lora/\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training and evaluation completed. Model and metrics saved in model-metric/{model_alias}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['conversation'].str.("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['issue_area'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
