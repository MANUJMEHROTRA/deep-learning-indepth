{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "69073400-6cee-4f3c-84f7-7278509eb66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft: 0.14.0\n",
      "Torch: 2.2.2\n",
      "Transformers: 4.49.0\n",
      "Accelerate: 1.4.0\n",
      "Huggingface Hub: 0.29.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "import accelerate\n",
    "import huggingface_hub\n",
    "import peft\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "import time\n",
    "import onnxruntime as ort\n",
    "import pickle\n",
    "\n",
    "print(\"peft:\", peft.__version__)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n",
    "print(\"Huggingface Hub:\", huggingface_hub.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0cab0edd-9b96-41e0-ba59-15ace279efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Quantization modules\n",
    "import torch.ao.quantization\n",
    "from torch.ao.quantization import quantize_dynamic, quantize, float_qparams_weight_only_qconfig\n",
    "from torch.ao.quantization import get_default_qconfig_mapping\n",
    "from torch.quantization.qconfig import QConfig\n",
    "from torch.ao.quantization import prepare, convert, QConfigMapping, default_dynamic_qconfig, default_qconfig\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0603dae5-889f-43ab-b3ec-816fe9af96b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple MPS GPU\n"
     ]
    }
   ],
   "source": [
    "# Device Selection Function\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using Apple MPS GPU\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using NVIDIA CUDA GPU\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "# Device Configuration\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "925717d6-d4d9-4af9-bd84-8d66d8b424aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_dict(model_alias, MODEL_NAME):\n",
    "    if not os.path.exists('model_dict.json'):\n",
    "        model_dict = {}\n",
    "    else:\n",
    "        with open('model_dict.json', 'r') as file:\n",
    "            model_dict = json.load(file)\n",
    "\n",
    "    model_dict[model_alias] = MODEL_NAME\n",
    "\n",
    "    with open('model_dict.json', 'w') as file:\n",
    "        json.dump(model_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "429fa2e6-0535-4cc0-a6d8-c1c91476ef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(filepath=\"./data/train-00000-of-00001-a5a7c6e4bb30b016.parquet\", model_alias=\"\"):\n",
    "    \"\"\"Loads and preprocesses the dataset.\"\"\"\n",
    "    df = pd.read_parquet(filepath)\n",
    "    df = df[['conversation', 'issue_area']]\n",
    "    print(\"Original distribution:\\n\", df['issue_area'].value_counts())\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"labels\"] = label_encoder.fit_transform(df[\"issue_area\"])\n",
    "\n",
    "    #saving Label-encoder\n",
    "    label_encoder_path = f\"model-metric/{model_alias}/label_encoder.pkl\"\n",
    "    os.makedirs(os.path.dirname(label_encoder_path), exist_ok=True)\n",
    "    with open(label_encoder_path, \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "        \n",
    "    return df, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "db74ce47-bb75-4776-8ef9-456074be9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(df, max_count=100, random_state=42):\n",
    "    \"\"\"Balances the dataset using oversampling.\"\"\"\n",
    "    balanced_df = pd.DataFrame()\n",
    "    for issue in df['issue_area'].unique():\n",
    "        subset = df[df['issue_area'] == issue]\n",
    "        balanced_subset = resample(subset, replace=True, n_samples=max_count, random_state=random_state)\n",
    "        balanced_df = pd.concat([balanced_df, balanced_subset])\n",
    "    return balanced_df.sample(frac=1, random_state=random_state).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fec3982e-d715-49e5-8009-c57e00de6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_conversation(conversation):\n",
    "    \"\"\"Preprocesses a conversation.\"\"\"\n",
    "    if isinstance(conversation, list):\n",
    "        return \" \".join([turn.get('text', '') for turn in conversation if isinstance(turn, dict)])\n",
    "    return str(conversation).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b6fce412-9089-43f4-b3c8-6065e441b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PyTorch Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        inputs = self.tokenizer(\n",
    "            row[\"conversation\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(0)\n",
    "        label = torch.tensor(row[\"labels\"], dtype=torch.long)\n",
    "        return input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8bb6320a-8fff-4375-b9cf-d345c2cdc420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(df, tokenizer, batch_size=8, train_ratio=0.75):\n",
    "    \"\"\"Creates train and test DataLoaders.\"\"\"\n",
    "    train_size = int(train_ratio * len(df))\n",
    "    train_df, test_df = df[:train_size], df[train_size:]\n",
    "    train_dataset = CustomDataset(train_df, tokenizer)\n",
    "    test_dataset = CustomDataset(test_df, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bb1885f5-7795-4c7e-a210-139f150861c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTWithLoRA(nn.Module):\n",
    "    def __init__(self, num_labels, lora_r=4, lora_alpha=16, lora_dropout=0.1):\n",
    "        super(DistilBERTWithLoRA, self).__init__()\n",
    "        # Load the base model with the correct number of labels\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"distilbert/distilbert-base-uncased\",\n",
    "            num_labels=num_labels  # Ensure this matches the number of classes\n",
    "        )\n",
    "        \n",
    "        # LoRA Configuration\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            target_modules=[\"q_lin\", \"k_lin\", \"v_lin\"]\n",
    "        )\n",
    "        self.bert = get_peft_model(self.bert, lora_config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits  # Return the logits directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2e409616-8331-4145-8cb3-198bfea1dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute class weights\n",
    "def compute_class_weights(labels, num_classes):\n",
    "    counter = Counter(labels)\n",
    "    total_samples = len(labels)\n",
    "    weights = [total_samples / (num_classes * counter[i]) for i in range(num_classes)]\n",
    "    return torch.tensor(weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e83e3ced-8839-4e5a-96bf-d249b88e8da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, model_alias, epochs=3, learning_rate=5e-5, class_weights=None):\n",
    "    \"\"\"Trains the model and saves logs, metrics, and model weights.\"\"\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Create directory for storing model metrics\n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # TensorBoard writer in the model directory\n",
    "    writer = SummaryWriter(log_dir=model_dir)\n",
    "\n",
    "    # Set up loss function and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights.to(device) if class_weights is not None else None)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    epoch_losses = []\n",
    "    metrics_data = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "            labels = labels.cpu().tolist()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "            # Log batch loss every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                writer.add_scalar(\"BatchLoss/train\", loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "        # Compute epoch metrics\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        # Store metrics for CSV logging\n",
    "        metrics_data.append([epoch + 1, avg_loss, accuracy, precision, recall, f1, epoch_time])\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Accuracy={accuracy:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1-score={f1:.4f}, Time={epoch_time:.2f}s\")\n",
    "\n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train\", accuracy, epoch)\n",
    "        writer.add_scalar(\"Precision/train\", precision, epoch)\n",
    "        writer.add_scalar(\"Recall/train\", recall, epoch)\n",
    "        writer.add_scalar(\"F1-score/train\", f1, epoch)\n",
    "        writer.add_scalar(\"Time/Epoch\", epoch_time, epoch)\n",
    "\n",
    "    # Save model KPIs as CSV\n",
    "    metrics_df = pd.DataFrame(metrics_data, columns=[\"Epoch\", \"Loss\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\", \"Time (s)\"])\n",
    "    metrics_df.to_csv(os.path.join(model_dir, \"training_metrics.csv\"), index=False)\n",
    "\n",
    "    # Save training loss curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, epochs + 1), epoch_losses, marker='o', linestyle='-', color='b')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    loss_plot_path = os.path.join(model_dir, \"training_loss.png\")\n",
    "    plt.savefig(loss_plot_path)\n",
    "    writer.add_figure(\"Training Loss\", plt.gcf(), close=True)\n",
    "\n",
    "    # Save model weights\n",
    "    model_path = os.path.join(model_dir, f\"{model_alias}.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "df386a15-bcfa-4586-a36f-8aa116dc1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, label_encoder, model_alias):\n",
    "    \"\"\"Evaluates the model and saves metrics, logs, and confusion matrix.\"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Create directory for storing model metrics\n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter(log_dir=model_dir)\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "            labels = labels.cpu().tolist()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    eval_time = time.time() - start_time\n",
    "    class_names = label_encoder.classes_\n",
    "\n",
    "    # Compute metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
    "    class_metrics = pd.DataFrame({\n",
    "        'Class': class_names,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Support': support\n",
    "    })\n",
    "\n",
    "    overall_precision, overall_recall, overall_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    # Print and save classification report\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save confusion matrix plot\n",
    "    confusion_matrix_path = os.path.join(model_dir, \"confusion_matrix.png\")\n",
    "    plt.savefig(confusion_matrix_path)\n",
    "    writer.add_figure(\"Confusion Matrix\", plt.gcf(), close=True)\n",
    "\n",
    "    # Print overall metrics\n",
    "    print(\"\\nPer-class Metrics:\\n\", class_metrics.to_string(index=False))\n",
    "    print(f\"\\nOverall Metrics:\\nPrecision: {overall_precision:.4f}, Recall: {overall_recall:.4f}, F1-score: {overall_f1:.4f}, Eval Time: {eval_time:.2f}s\")\n",
    "\n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar(\"Precision/test\", overall_precision)\n",
    "    writer.add_scalar(\"Recall/test\", overall_recall)\n",
    "    writer.add_scalar(\"F1-score/test\", overall_f1)\n",
    "    writer.add_scalar(\"Evaluation Time\", eval_time)\n",
    "\n",
    "    # Log per-class metrics\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        writer.add_scalar(f\"Precision/{class_name}\", precision[i])\n",
    "        writer.add_scalar(f\"Recall/{class_name}\", recall[i])\n",
    "        writer.add_scalar(f\"F1-score/{class_name}\", f1[i])\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "    # Save evaluation metrics\n",
    "    class_metrics.to_csv(os.path.join(model_dir, \"class_metrics.csv\"), index=False)\n",
    "    cm_df.to_csv(os.path.join(model_dir, \"confusion_matrix.csv\"))\n",
    "\n",
    "    return overall_precision, overall_recall, overall_f1, eval_time, class_metrics, cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bdcc158e-5639-4255-a520-64874eb10283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_onnx(model, tokenizer, model_alias):\n",
    "    \"\"\"Exports the model to ONNX format.\"\"\"\n",
    "    model.eval().to(\"cpu\")\n",
    "    sample_input = tokenizer(\"test\", return_tensors=\"pt\")\n",
    "    input_names = [\"input_ids\", \"attention_mask\"]\n",
    "    output_names = [\"output\"]\n",
    "    \n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    onnx_path = os.path.join(model_dir, f\"{model_alias}.onnx\")\n",
    "    \n",
    "    torch.onnx.export(\n",
    "        model, \n",
    "        (sample_input[\"input_ids\"], sample_input[\"attention_mask\"]), \n",
    "        onnx_path, \n",
    "        input_names=input_names, \n",
    "        output_names=output_names, \n",
    "        dynamic_axes={\n",
    "            \"input_ids\": {0: \"batch\", 1: \"sequence\"}, \n",
    "            \"attention_mask\": {0: \"batch\", 1: \"sequence\"}, \n",
    "            \"output\": {0: \"batch\"}\n",
    "        }\n",
    "    )\n",
    "    print(f\"ONNX model exported to {onnx_path}\")\n",
    "    return onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "952ef9de-2950-4498-9146-416163925ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model, model_path=None):\n",
    "    \"\"\"Get the size of the model in MB.\"\"\"\n",
    "    if model_path:\n",
    "        # Get the size of the saved model file\n",
    "        size_bytes = os.path.getsize(model_path)\n",
    "    else:\n",
    "        # Estimate the size in memory\n",
    "        param_size = 0\n",
    "        for param in model.parameters():\n",
    "            param_size += param.nelement() * param.element_size()\n",
    "        size_bytes = param_size\n",
    "    \n",
    "    # Convert to MB\n",
    "    size_mb = size_bytes / (1024 * 1024)\n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467a4afb-8a36-4164-82f3-4859fc4c83b8",
   "metadata": {},
   "source": [
    "### Post-Training Quantization (PTQ) Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cd4e7402-740c-4ba6-adf5-2ec54ae59663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dynamic_quantization(model, model_alias):\n",
    "    \"\"\"Apply dynamic quantization to the model, handling LoRA layers properly.\"\"\"\n",
    "    # Make a copy of the model for quantization\n",
    "    quantized_model = copy.deepcopy(model)\n",
    "    quantized_model.eval()\n",
    "    \n",
    "    # Move to CPU as quantization is only supported on CPU\n",
    "    quantized_model.to(\"cpu\")\n",
    "    \n",
    "    # Set the quantization backend\n",
    "    import platform\n",
    "    if platform.machine() in ['x86_64', 'AMD64']:\n",
    "        torch.backends.quantized.engine = 'fbgemm'\n",
    "    else:\n",
    "        torch.backends.quantized.engine = 'qnnpack'\n",
    "    \n",
    "    print(f\"Using quantization engine: {torch.backends.quantized.engine}\")\n",
    "    \n",
    "    # Check if this is a PEFT LoRA model\n",
    "    is_lora_model = hasattr(quantized_model, 'base_model') and hasattr(quantized_model, 'peft_config')\n",
    "    \n",
    "    if is_lora_model:\n",
    "        print(\"Detected PEFT LoRA model - using specialized quantization approach\")\n",
    "        \n",
    "        # For LoRA models, we need to be careful not to quantize the LoRA layers\n",
    "        # We'll only quantize the base model's linear layers\n",
    "        for name, module in quantized_model.base_model.named_modules():\n",
    "            # Skip LoRA layers and only quantize standard linear layers\n",
    "            if isinstance(module, torch.nn.Linear) and not hasattr(module, 'lora_A'):\n",
    "                # Use torch.quantization.quantize_dynamic for individual layers\n",
    "                quantized_layer = torch.ao.quantization.quantize_dynamic(\n",
    "                    module, \n",
    "                    {torch.nn.Linear}, \n",
    "                    dtype=torch.qint8\n",
    "                )\n",
    "                # Replace the original layer with the quantized one\n",
    "                parent = quantized_model.base_model\n",
    "                path = name.split('.')\n",
    "                for p in path[:-1]:\n",
    "                    parent = getattr(parent, p)\n",
    "                setattr(parent, path[-1], quantized_layer)\n",
    "    else:\n",
    "        # For non-LoRA models, use standard dynamic quantization\n",
    "        try:\n",
    "            quantized_model = torch.ao.quantization.quantize_dynamic(\n",
    "                quantized_model, \n",
    "                {torch.nn.Linear}, \n",
    "                dtype=torch.qint8\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Standard dynamic quantization failed: {e}\")\n",
    "            print(\"Falling back to manual weight quantization...\")\n",
    "            \n",
    "            # Manual approach to quantize weights\n",
    "            for name, module in quantized_model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    with torch.no_grad():\n",
    "                        # Simulate INT8 quantization\n",
    "                        weight = module.weight.data\n",
    "                        scale = max(weight.abs().max() / 127, 1e-8)\n",
    "                        quant_weight = (weight / scale).round().clamp(-127, 127) * scale\n",
    "                        module.weight.data = quant_weight\n",
    "    \n",
    "    # Save the quantized model\n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_path = os.path.join(model_dir, f\"{model_alias}.pth\")\n",
    "    torch.save(quantized_model.state_dict(), model_path)\n",
    "    \n",
    "    print(f\"LoRA-compatible quantized model saved to {model_path}\")\n",
    "    return quantized_model, model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "bd4f737c-efcf-4624-b8ec-67ffb7bd7308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_static_quantization(model, calibration_loader, model_alias):\n",
    "    \"\"\"\n",
    "    Apply static quantization to the model.\n",
    "    Static quantization requires a calibration step with representative data.\n",
    "    \"\"\"\n",
    "    # Make a copy of the model for quantization\n",
    "    quantized_model = copy.deepcopy(model)\n",
    "    \n",
    "    # Set the quantization engine based on hardware\n",
    "    import platform\n",
    "    if platform.machine() in ['x86_64', 'AMD64']:\n",
    "        torch.backends.quantized.engine = 'fbgemm'\n",
    "    else:\n",
    "        torch.backends.quantized.engine = 'qnnpack'\n",
    "    \n",
    "    quantized_model.eval()\n",
    "    \n",
    "    # Move to CPU as quantization is only supported on CPU\n",
    "    quantized_model.to(\"cpu\")\n",
    "    \n",
    "    try:\n",
    "        # Define qconfig mapping\n",
    "        qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
    "        qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "        \n",
    "        # Prepare model for calibration\n",
    "        prepared_model = torch.quantization.prepare(quantized_model, qconfig_mapping)\n",
    "        \n",
    "        # Calibrate with a subset of data\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(calibration_loader):\n",
    "                input_ids, attention_mask, _ = batch\n",
    "                prepared_model(input_ids, attention_mask)\n",
    "                # Use only a small subset for calibration\n",
    "                if batch_idx >= 10:\n",
    "                    break\n",
    "        \n",
    "        # Convert to quantized model\n",
    "        quantized_model = torch.quantization.convert(prepared_model)\n",
    "    except Exception as e:\n",
    "        print(f\"Standard static quantization failed: {e}\")\n",
    "        print(\"Using simplified static quantization...\")\n",
    "        \n",
    "        # Apply manual quantization to supported modules\n",
    "        for name, module in quantized_model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                try:\n",
    "                    # Get the parent module\n",
    "                    module_path = name.split('.')\n",
    "                    parent_module = quantized_model\n",
    "                    for path_item in module_path[:-1]:\n",
    "                        parent_module = getattr(parent_module, path_item)\n",
    "                    \n",
    "                    # Replace with a quantized version if possible\n",
    "                    q_module = torch.nn.quantized.Linear.from_float(module)\n",
    "                    setattr(parent_module, module_path[-1], q_module)\n",
    "                except Exception as nested_e:\n",
    "                    print(f\"Couldn't quantize module {name}: {nested_e}\")\n",
    "    \n",
    "    # Save the quantized model\n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_path = os.path.join(model_dir, f\"{model_alias}.pth\")\n",
    "    torch.save(quantized_model.state_dict(), model_path)\n",
    "    \n",
    "    print(f\"Static quantized model saved to {model_path}\")\n",
    "    return quantized_model, model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "69cfe1d6-9c28-48af-961d-6f244f9d3e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_weight_only_quantization(model, model_alias):\n",
    "    \"\"\"Apply weight-only quantization to the model.\"\"\"\n",
    "    # Make a copy of the model for quantization\n",
    "    quantized_model = copy.deepcopy(model)\n",
    "    quantized_model.eval()\n",
    "    \n",
    "    # Set the quantization engine based on hardware\n",
    "    import platform\n",
    "    if platform.machine() in ['x86_64', 'AMD64']:\n",
    "        torch.backends.quantized.engine = 'fbgemm'\n",
    "    else:\n",
    "        torch.backends.quantized.engine = 'qnnpack'\n",
    "    \n",
    "    # Move to CPU as quantization is only supported on CPU\n",
    "    quantized_model.to(\"cpu\")\n",
    "    \n",
    "    try:\n",
    "        # Set qconfig for weight-only quantization\n",
    "        qconfig = float_qparams_weight_only_qconfig\n",
    "        qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "        \n",
    "        # Prepare and convert model\n",
    "        prepared_model = torch.quantization.prepare(quantized_model, qconfig_mapping)\n",
    "        quantized_model = torch.quantization.convert(prepared_model)\n",
    "    except Exception as e:\n",
    "        print(f\"Standard weight-only quantization failed: {e}\")\n",
    "        print(\"Implementing manual weight quantization...\")\n",
    "        \n",
    "        # Manual weight quantization approach\n",
    "        for name, module in quantized_model.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                try:\n",
    "                    # Get the parent module\n",
    "                    module_path = name.split('.')\n",
    "                    parent_module = quantized_model\n",
    "                    for path_item in module_path[:-1]:\n",
    "                        parent_module = getattr(parent_module, path_item)\n",
    "                    \n",
    "                    # Apply weight quantization manually\n",
    "                    # Scale weights to int8 range and back to simulate quantization\n",
    "                    with torch.no_grad():\n",
    "                        weight = module.weight.data\n",
    "                        # Simple min-max quantization to simulate INT8 precision\n",
    "                        scale = max(weight.abs().max() / 127, 1e-8)\n",
    "                        quant_weight = (weight / scale).round().clamp(-127, 127) * scale\n",
    "                        module.weight.data = quant_weight\n",
    "                except Exception as nested_e:\n",
    "                    print(f\"Couldn't quantize weights for module {name}: {nested_e}\")\n",
    "    \n",
    "    # Save the quantized model\n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_path = os.path.join(model_dir, f\"{model_alias}.pth\")\n",
    "    torch.save(quantized_model.state_dict(), model_path)\n",
    "    \n",
    "    print(f\"Weight-only quantized model saved to {model_path}\")\n",
    "    return quantized_model, model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8125b4fd-c376-4ae0-ab44-766032c84320",
   "metadata": {},
   "source": [
    "## Quantization-Aware Training (QAT) Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4b5ffcc5-f0d8-4eba-b3ef-c5101530a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizationAwareTraining:\n",
    "    def __init__(self, model, train_loader, test_loader, label_encoder, model_alias, epochs=3, learning_rate=5e-5, class_weights=None):\n",
    "        self.model = copy.deepcopy(model)\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.label_encoder = label_encoder\n",
    "        self.model_alias = model_alias\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.class_weights = class_weights\n",
    "        self.device = torch.device(\"cpu\")  # QAT requires CPU\n",
    "        \n",
    "        # Create directory for model metrics\n",
    "        self.model_dir = f\"model-metric/{model_alias}\"\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "        \n",
    "        # TensorBoard writer\n",
    "        self.writer = SummaryWriter(log_dir=self.model_dir)\n",
    "    \n",
    "    def prepare_qat_model(self):\n",
    "        \"\"\"Prepare model for Quantization-Aware Training.\"\"\"\n",
    "        # Set platform specific engine\n",
    "        import platform\n",
    "        if platform.machine() in ['x86_64', 'AMD64']:\n",
    "            torch.backends.quantized.engine = 'fbgemm'\n",
    "        else:\n",
    "            torch.backends.quantized.engine = 'qnnpack'\n",
    "            \n",
    "        # Set model to train mode\n",
    "        self.model.train()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Define qconfig for QAT\n",
    "        from torch.quantization import QConfig\n",
    "        from torch.quantization.observer import MovingAverageMinMaxObserver\n",
    "        from torch.quantization.fake_quantize import FakeQuantize\n",
    "        \n",
    "        qconfig = QConfig(\n",
    "            activation=FakeQuantize.with_args(\n",
    "                observer=MovingAverageMinMaxObserver,\n",
    "                quant_min=0,\n",
    "                quant_max=255,\n",
    "                dtype=torch.quint8,\n",
    "                qscheme=torch.per_tensor_affine,\n",
    "                reduce_range=False),\n",
    "            weight=FakeQuantize.with_args(\n",
    "                observer=MovingAverageMinMaxObserver,\n",
    "                quant_min=-128,\n",
    "                quant_max=127,\n",
    "                dtype=torch.qint8,\n",
    "                qscheme=torch.per_tensor_symmetric,\n",
    "                reduce_range=False)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def train_qat_model(self):\n",
    "        \"\"\"Train the model with Quantization-Aware Training.\"\"\"\n",
    "        # Fallback in case prepare_qat_model wasn't run first\n",
    "        if not hasattr(self, 'qat_model'):\n",
    "            self.prepare_qat_model()\n",
    "            \n",
    "        # Set up loss function and optimizer\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(self.device) if self.class_weights is not None else None)\n",
    "        optimizer = torch.optim.AdamW(self.qat_model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        epoch_losses = []\n",
    "        metrics_data = []\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            start_time = time.time()\n",
    "            total_loss = 0\n",
    "            all_preds, all_labels = [], []\n",
    "            \n",
    "            for batch_idx, batch in enumerate(self.train_loader):\n",
    "                input_ids, attention_mask, labels = [x.to(self.device) for x in batch]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                try:\n",
    "                    outputs = self.qat_model(input_ids, attention_mask=attention_mask)\n",
    "                    # If outputs is a dictionary, get the logits\n",
    "                    if isinstance(outputs, dict) and 'logits' in outputs:\n",
    "                        outputs = outputs['logits']\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during forward pass: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                preds = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "                labels = labels.cpu().tolist()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels)\n",
    "                \n",
    "                # Log batch loss every 10 batches\n",
    "                if batch_idx % 10 == 0:\n",
    "                    self.writer.add_scalar(\"BatchLoss/qat_train\", loss.item(), epoch * len(self.train_loader) + batch_idx)\n",
    "            \n",
    "            # Compute epoch metrics\n",
    "            avg_loss = total_loss / len(self.train_loader)\n",
    "            epoch_losses.append(avg_loss)\n",
    "            accuracy = accuracy_score(all_labels, all_preds)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "            epoch_time = time.time() - start_time\n",
    "            \n",
    "            # Store metrics for CSV logging\n",
    "            metrics_data.append([epoch + 1, avg_loss, accuracy, precision, recall, f1, epoch_time])\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"QAT Epoch {epoch+1}: Loss={avg_loss:.4f}, Accuracy={accuracy:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1-score={f1:.4f}, Time={epoch_time:.2f}s\")\n",
    "            \n",
    "            # Log metrics to TensorBoard\n",
    "            self.writer.add_scalar(\"Loss/qat_train\", avg_loss, epoch)\n",
    "            self.writer.add_scalar(\"Accuracy/qat_train\", accuracy, epoch)\n",
    "            self.writer.add_scalar(\"Precision/qat_train\", precision, epoch)\n",
    "            self.writer.add_scalar(\"Recall/qat_train\", recall, epoch)\n",
    "            self.writer.add_scalar(\"F1-score/qat_train\", f1, epoch)\n",
    "            self.writer.add_scalar(\"Time/Epoch_qat\", epoch_time, epoch)\n",
    "        \n",
    "        # Save model KPIs as CSV\n",
    "        metrics_df = pd.DataFrame(metrics_data, columns=[\"Epoch\", \"Loss\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\", \"Time (s)\"])\n",
    "        metrics_df.to_csv(os.path.join(self.model_dir, \"qat_training_metrics.csv\"), index=False)\n",
    "        \n",
    "        # Save training loss curve\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, self.epochs + 1), epoch_losses, marker='o', linestyle='-', color='b')\n",
    "        plt.title('QAT Training Loss Over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        loss_plot_path = os.path.join(self.model_dir, \"qat_training_loss.png\")\n",
    "        plt.savefig(loss_plot_path)\n",
    "        self.writer.add_figure(\"QAT Training Loss\", plt.gcf(), close=True)\n",
    "        \n",
    "        # Create a simple quantized version of the model\n",
    "        # Since full conversion might fail, we'll use a simpler approach\n",
    "        quantized_model = copy.deepcopy(self.qat_model)\n",
    "        \n",
    "        try:\n",
    "            # Try standard conversion\n",
    "            quantized_model = torch.quantization.convert(self.qat_model.eval(), inplace=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Standard QAT conversion failed: {e}\")\n",
    "            print(\"Using simplified model instead...\")\n",
    "            # If conversion fails, just use the trained model\n",
    "            quantized_model = self.qat_model.eval()\n",
    "        \n",
    "        # Save model weights\n",
    "        model_path = os.path.join(self.model_dir, f\"{self.model_alias}.pth\")\n",
    "        torch.save(quantized_model.state_dict(), model_path)\n",
    "        \n",
    "        self.writer.flush()\n",
    "        self.writer.close()\n",
    "        \n",
    "        self.quantized_model = quantized_model\n",
    "        return quantized_model, model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205a0202-fd78-4a01-96ec-cbe16e9b96c9",
   "metadata": {},
   "source": [
    "### Comparison Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "558d2f7e-fe5c-40d6-aeb4-e47edd082437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_latency(model, test_loader, num_batches=100):\n",
    "    \"\"\"Measure inference latency for a model.\"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cpu\")  # Ensure we're on CPU for fair comparison\n",
    "    model.to(device)\n",
    "    \n",
    "    latencies = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            if batch_idx >= num_batches:\n",
    "                break\n",
    "                \n",
    "            input_ids, attention_mask, _ = [x.to(device) for x in batch]\n",
    "            \n",
    "            # Warm-up run\n",
    "            _ = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Timed run\n",
    "            start_time = time.time()\n",
    "            _ = model(input_ids, attention_mask=attention_mask)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            latencies.append(end_time - start_time)\n",
    "    \n",
    "    # Calculate average latency in milliseconds\n",
    "    avg_latency_ms = np.mean(latencies) * 1000\n",
    "    return avg_latency_ms\n",
    "\n",
    "def compare_models(models_info):\n",
    "    \"\"\"Compare multiple models and create visualizations.\"\"\"\n",
    "    # Create a DataFrame for comparison\n",
    "    comparison_df = pd.DataFrame(models_info)\n",
    "    \n",
    "    # Save comparison to CSV\n",
    "    comparison_df.to_csv(\"model_comparison.csv\", index=False)\n",
    "    \n",
    "    # Create bar charts for visual comparison\n",
    "    metrics = ['Size (MB)', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Latency (ms)']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(comparison_df['Model'], comparison_df[metric])\n",
    "        plt.xlabel('Model')\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f'Comparison of {metric} Across Models')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add values on top of bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{height:.2f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"comparison_{metric.replace(' ', '_').lower()}.png\")\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    return comparison_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc60db1-353c-47d0-a68c-e344abc3e6b9",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ebcd3-b535-40d1-b0c2-b530f30b2feb",
   "metadata": {},
   "source": [
    "### Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e73ce935-1942-49a3-a178-a3ae1db230ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = \"distilbert/distilbert-base-uncased\"\n",
    "\n",
    "# Define model aliases\n",
    "original_model_alias = 'distilbert-original'\n",
    "dynamic_ptq_model_alias = 'distilbert-dynamic-ptq'\n",
    "static_ptq_model_alias = 'distilbert-static-ptq'\n",
    "weight_only_ptq_model_alias = 'distilbert-weight-only-ptq'\n",
    "qat_model_alias = 'distilbert-qat'\n",
    "\n",
    "update_model_dict(original_model_alias, MODEL_NAME)\n",
    "update_model_dict(dynamic_ptq_model_alias, MODEL_NAME)\n",
    "update_model_dict(static_ptq_model_alias, MODEL_NAME)\n",
    "update_model_dict(weight_only_ptq_model_alias, MODEL_NAME)\n",
    "update_model_dict(qat_model_alias, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5af84a8-5381-423b-9bf1-6666f7083b01",
   "metadata": {},
   "source": [
    "### Step 2: Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a73cf6d1-65a9-4cb5-93a5-d8105aa04388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution:\n",
      " issue_area\n",
      "Cancellations and returns    286\n",
      "Order                        270\n",
      "Login and Account            151\n",
      "Shopping                     116\n",
      "Warranty                     105\n",
      "Shipping                      72\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df, label_encoder = load_and_preprocess_data(model_alias=original_model_alias)\n",
    "balanced_df = balance_dataset(df)\n",
    "balanced_df['conversation'] = balanced_df['conversation'].apply(preprocess_conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194b6d6a-a491-4ea2-b63f-24efdac2747a",
   "metadata": {},
   "source": [
    "### Step 3: Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "26c8e11f-f120-4638-abf6-5b10bd65255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "train_loader, test_loader, test_df = create_dataloaders(balanced_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164fda88-fae2-413c-bf7e-4e9a2700b05e",
   "metadata": {},
   "source": [
    "### Step 4: Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c39d24c7-063c-43b7-9146-b6c202f489fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(label_encoder.classes_)\n",
    "model = DistilBERTWithLoRA(num_labels=num_classes)\n",
    "class_weights = compute_class_weights(balanced_df['labels'], num_classes)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d097bb9e-e17d-4156-9d6e-aefb60e08759",
   "metadata": {},
   "source": [
    "### Step 5: Train and Evaluate Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "05ebbefc-65de-4b8f-a2ff-4c3e30bf3a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Original Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manmehro1/Desktop/repository/lora-training-explaination/verizon-poc/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=1.7866, Accuracy=0.1667, Precision=0.0851, Recall=0.1667, F1-score=0.1044, Time=151.90s\n",
      "Epoch 2: Loss=1.7209, Accuracy=0.3867, Precision=0.4948, Recall=0.3867, F1-score=0.3630, Time=149.32s\n",
      "Epoch 3: Loss=1.3196, Accuracy=0.6756, Precision=0.7120, Recall=0.6756, F1-score=0.6481, Time=133.35s\n",
      "Epoch 4: Loss=0.7957, Accuracy=0.7844, Precision=0.7841, Recall=0.7844, F1-score=0.7706, Time=133.53s\n",
      "Epoch 5: Loss=0.5795, Accuracy=0.8444, Precision=0.8430, Recall=0.8444, F1-score=0.8385, Time=137.46s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Training Original Model ===\")\n",
    "train_model(model, train_loader, model_alias=original_model_alias, epochs=5, learning_rate=5e-5, class_weights=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1c6dfcf9-443b-4b7a-b783-fb7b8e358973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save original model\n",
    "original_model_path = f\"model-metric/{original_model_alias}/{original_model_alias}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3bf4ff3a-7e1f-4d6e-bd57-f1a21fe27127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Original Model ===\n",
      "\n",
      "Classification Report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "Cancellations and returns       0.82      0.90      0.86        20\n",
      "        Login and Account       1.00      0.96      0.98        28\n",
      "                    Order       0.84      0.70      0.76        30\n",
      "                 Shipping       0.74      1.00      0.85        23\n",
      "                 Shopping       1.00      0.82      0.90        22\n",
      "                 Warranty       1.00      1.00      1.00        27\n",
      "\n",
      "                 accuracy                           0.89       150\n",
      "                macro avg       0.90      0.90      0.89       150\n",
      "             weighted avg       0.90      0.89      0.89       150\n",
      "\n",
      "\n",
      "Per-class Metrics:\n",
      "                     Class  Precision   Recall  F1-Score  Support\n",
      "Cancellations and returns   0.818182 0.900000  0.857143       20\n",
      "        Login and Account   1.000000 0.964286  0.981818       28\n",
      "                    Order   0.840000 0.700000  0.763636       30\n",
      "                 Shipping   0.741935 1.000000  0.851852       23\n",
      "                 Shopping   1.000000 0.818182  0.900000       22\n",
      "                 Warranty   1.000000 1.000000  1.000000       27\n",
      "\n",
      "Overall Metrics:\n",
      "Precision: 0.9042, Recall: 0.8933, F1-score: 0.8929, Eval Time: 10.50s\n"
     ]
    }
   ],
   "source": [
    "# Evaluate original model\n",
    "print(\"\\n=== Evaluating Original Model ===\")\n",
    "original_precision, original_recall, original_f1, original_eval_time, _, _ = evaluate_model(\n",
    "    model, test_loader, label_encoder, original_model_alias\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d393166c-7a84-4219-acdf-d4500e10c13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_size: 258.19445991516113\n",
      "original_latency: 779.9079418182373\n"
     ]
    }
   ],
   "source": [
    "# Measure model size and latency\n",
    "original_size = get_model_size(model, original_model_path)\n",
    "original_latency = measure_inference_latency(model, test_loader)\n",
    "\n",
    "print(f\"original_size: {original_size}\")\n",
    "print(f\"original_latency: {original_latency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7bf7734b-8478-4cef-8857-e4af62bb0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy from test set\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = [x.to(\"cpu\") for x in batch]\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "        labels = labels.cpu().tolist()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "original_accuracy = accuracy_score(all_labels, all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a340fb27-152b-4258-a587-106e82c28de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_accuracy: 0.8933333333333333\n"
     ]
    }
   ],
   "source": [
    "print(f\"original_accuracy: {original_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db36f7b-7a24-433b-8b56-191ade48df9f",
   "metadata": {},
   "source": [
    "### Step 6: Apply Dynamic Post-Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a65babf0-f697-43b4-a179-43d91a50a30b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"\\n=== Applying Dynamic Post-Training Quantization ===\")\n",
    "# dynamic_quantized_model, dynamic_quantized_path = apply_dynamic_quantization(model, dynamic_ptq_model_alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "168bac3c-4103-45ce-b8c2-635f8f8c7af7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Evaluate dynamic quantized model\n",
    "# print(\"\\n=== Evaluating Dynamic Quantized Model ===\")\n",
    "# dynamic_precision, dynamic_recall, dynamic_f1, dynamic_eval_time, _, _ = evaluate_model(\n",
    "#     dynamic_quantized_model, test_loader, label_encoder, dynamic_ptq_model_alias\n",
    "# )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b292dcc1-3a9d-4b12-9fe7-854b082ae548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Measure model size and latency\n",
    "# dynamic_size = get_model_size(dynamic_quantized_model, dynamic_quantized_path)\n",
    "# dynamic_latency = measure_inference_latency(dynamic_quantized_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7ce99a47-e7d4-41ca-9d96-88f59879669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get accuracy from test set\n",
    "# dynamic_quantized_model.eval()\n",
    "# all_preds, all_labels = [], []\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_loader:\n",
    "#         input_ids, attention_mask, labels = [x.to(\"cpu\") for x in batch]\n",
    "#         outputs = dynamic_quantized_model(input_ids, attention_mask=attention_mask)\n",
    "#         preds = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "#         labels = labels.cpu().tolist()\n",
    "#         all_preds.extend(preds)\n",
    "#         all_labels.extend(labels)\n",
    "# dynamic_accuracy = accuracy_score(all_labels, all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac4b1b-9b40-45ff-bc46-3db5e5edb5ec",
   "metadata": {},
   "source": [
    "### Step 7: Apply Static Post-Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3448a69d-f1e7-41a0-8e59-a1d6c17137d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Applying Static Post-Training Quantization ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manmehro1/Desktop/repository/lora-training-explaination/verizon-poc/.venv/lib/python3.12/site-packages/torch/ao/quantization/quantize.py:312: UserWarning: None of the submodule got qconfig applied. Make sure you passed correct configuration through `qconfig_dict` or by assigning the `.qconfig` attribute directly on submodules\n",
      "  warnings.warn(\"None of the submodule got qconfig applied. Make sure you \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static quantized model saved to model-metric/distilbert-static-ptq/distilbert-static-ptq.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== Applying Static Post-Training Quantization ===\")\n",
    "# Use a subset of train_loader for calibration\n",
    "calibration_loader = DataLoader(\n",
    "    CustomDataset(balanced_df.sample(100), tokenizer),\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")\n",
    "static_quantized_model, static_quantized_path = apply_static_quantization(model, calibration_loader, static_ptq_model_alias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ff347728-6b02-4596-a92e-521a99ef63f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Static Quantized Model ===\n",
      "\n",
      "Classification Report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "Cancellations and returns       0.82      0.90      0.86        20\n",
      "        Login and Account       1.00      0.96      0.98        28\n",
      "                    Order       0.84      0.70      0.76        30\n",
      "                 Shipping       0.74      1.00      0.85        23\n",
      "                 Shopping       1.00      0.82      0.90        22\n",
      "                 Warranty       1.00      1.00      1.00        27\n",
      "\n",
      "                 accuracy                           0.89       150\n",
      "                macro avg       0.90      0.90      0.89       150\n",
      "             weighted avg       0.90      0.89      0.89       150\n",
      "\n",
      "\n",
      "Per-class Metrics:\n",
      "                     Class  Precision   Recall  F1-Score  Support\n",
      "Cancellations and returns   0.818182 0.900000  0.857143       20\n",
      "        Login and Account   1.000000 0.964286  0.981818       28\n",
      "                    Order   0.840000 0.700000  0.763636       30\n",
      "                 Shipping   0.741935 1.000000  0.851852       23\n",
      "                 Shopping   1.000000 0.818182  0.900000       22\n",
      "                 Warranty   1.000000 1.000000  1.000000       27\n",
      "\n",
      "Overall Metrics:\n",
      "Precision: 0.9042, Recall: 0.8933, F1-score: 0.8929, Eval Time: 10.75s\n"
     ]
    }
   ],
   "source": [
    "# Evaluate static quantized model\n",
    "print(\"\\n=== Evaluating Static Quantized Model ===\")\n",
    "static_precision, static_recall, static_f1, static_eval_time, _, _ = evaluate_model(\n",
    "    static_quantized_model, test_loader, label_encoder, static_ptq_model_alias\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2a26db0c-00f1-4493-b09a-fb790afd30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure model size and latency\n",
    "static_size = get_model_size(static_quantized_model, static_quantized_path)\n",
    "static_latency = measure_inference_latency(static_quantized_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "da30eb79-c631-404e-b618-5dd12d3896d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_size: 258.1947422027588\n",
      "static_latency: 552.5903199848375\n"
     ]
    }
   ],
   "source": [
    "print(f\"static_size: {static_size}\")\n",
    "print(f\"static_latency: {static_latency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1a0b4b39-9378-423f-bed6-b4f4aa3f3fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Get accuracy from test set\n",
    "static_quantized_model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = [x.to(\"cpu\") for x in batch]\n",
    "        outputs = static_quantized_model(input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "        labels = labels.cpu().tolist()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "static_accuracy = accuracy_score(all_labels, all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f122114b-b210-4e9b-a2a8-188b57f412e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_accuracy: 0.8933333333333333\n"
     ]
    }
   ],
   "source": [
    "print(f\"static_accuracy: {static_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f472c160-1172-4d94-954a-ad883ab39281",
   "metadata": {},
   "source": [
    "### Step 8: Apply Weight-Only Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d20092ac-6d6f-4c0f-9efb-c9eea309807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Applying Weight-Only Quantization ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manmehro1/Desktop/repository/lora-training-explaination/verizon-poc/.venv/lib/python3.12/site-packages/torch/ao/quantization/quantize.py:312: UserWarning: None of the submodule got qconfig applied. Make sure you passed correct configuration through `qconfig_dict` or by assigning the `.qconfig` attribute directly on submodules\n",
      "  warnings.warn(\"None of the submodule got qconfig applied. Make sure you \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight-only quantized model saved to model-metric/distilbert-weight-only-ptq/distilbert-weight-only-ptq.pth\n",
      "\n",
      "=== Evaluating Weight-Only Quantized Model ===\n",
      "\n",
      "Classification Report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "Cancellations and returns       0.82      0.90      0.86        20\n",
      "        Login and Account       1.00      0.96      0.98        28\n",
      "                    Order       0.84      0.70      0.76        30\n",
      "                 Shipping       0.74      1.00      0.85        23\n",
      "                 Shopping       1.00      0.82      0.90        22\n",
      "                 Warranty       1.00      1.00      1.00        27\n",
      "\n",
      "                 accuracy                           0.89       150\n",
      "                macro avg       0.90      0.90      0.89       150\n",
      "             weighted avg       0.90      0.89      0.89       150\n",
      "\n",
      "\n",
      "Per-class Metrics:\n",
      "                     Class  Precision   Recall  F1-Score  Support\n",
      "Cancellations and returns   0.818182 0.900000  0.857143       20\n",
      "        Login and Account   1.000000 0.964286  0.981818       28\n",
      "                    Order   0.840000 0.700000  0.763636       30\n",
      "                 Shipping   0.741935 1.000000  0.851852       23\n",
      "                 Shopping   1.000000 0.818182  0.900000       22\n",
      "                 Warranty   1.000000 1.000000  1.000000       27\n",
      "\n",
      "Overall Metrics:\n",
      "Precision: 0.9042, Recall: 0.8933, F1-score: 0.8929, Eval Time: 10.95s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Applying Weight-Only Quantization ===\")\n",
    "weight_only_model, weight_only_path = apply_weight_only_quantization(model, weight_only_ptq_model_alias)\n",
    "\n",
    "# Evaluate weight-only quantized model\n",
    "print(\"\\n=== Evaluating Weight-Only Quantized Model ===\")\n",
    "weight_only_precision, weight_only_recall, weight_only_f1, weight_only_eval_time, _, _ = evaluate_model(\n",
    "    weight_only_model, test_loader, label_encoder, weight_only_ptq_model_alias\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8cf2c5d2-df4f-465c-9a4d-0a32ce9b27e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure model size and latency\n",
    "weight_only_size = get_model_size(weight_only_model, weight_only_path)\n",
    "weight_only_latency = measure_inference_latency(weight_only_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c7840eb0-564c-4e67-9139-c8d415f63975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_only_size: 258.1955089569092\n",
      "weight_only_latency: 563.8014391848916\n"
     ]
    }
   ],
   "source": [
    "print(f\"weight_only_size: {weight_only_size}\")\n",
    "print(f\"weight_only_latency: {weight_only_latency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8050eeff-ed05-4586-b37c-0c8946b128b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy from test set\n",
    "weight_only_model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = [x.to(\"cpu\") for x in batch]\n",
    "        outputs = weight_only_model(input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "        labels = labels.cpu().tolist()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "weight_only_accuracy = accuracy_score(all_labels, all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8bede481-5356-4d7a-9913-2f3b2ea5ce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_only_accuracy: 0.8933333333333333\n"
     ]
    }
   ],
   "source": [
    "print(f\"weight_only_accuracy: {weight_only_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317ab9b7-9b2a-4b01-8dd5-9509977ffe30",
   "metadata": {},
   "source": [
    " ### Step 9: Apply Quantization-Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bf3b43fc-e2ad-489f-a113-ef3927d0ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"\\n=== Applying Quantization-Aware Training ===\")\n",
    "# qat = QuantizationAwareTraining(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     test_loader=test_loader,\n",
    "#     label_encoder=label_encoder,\n",
    "#     model_alias=qat_model_alias,\n",
    "#     epochs=5,\n",
    "#     learning_rate=5e-5,\n",
    "#     class_weights=class_weights\n",
    "# )\n",
    "# qat_model = qat.prepare_qat_model()\n",
    "# qat_quantized_model, qat_path = qat.train_qat_model()\n",
    "\n",
    "# # Evaluate QAT model\n",
    "# print(\"\\n=== Evaluating QAT Model ===\")\n",
    "# qat_precision, qat_recall, qat_f1, qat_eval_time, _, _ = evaluate_model(\n",
    "#     qat_quantized_model, test_loader, label_encoder, qat_model_alias\n",
    "# )\n",
    "\n",
    "# # Measure model size and latency\n",
    "# qat_size = get_model_size(qat_quantized_model, qat_path)\n",
    "# qat_latency = measure_inference_latency(qat_quantized_model, test_loader)\n",
    "\n",
    "# # Get accuracy from test set\n",
    "# qat_quantized_model.eval()\n",
    "# all_preds, all_labels = [], []\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_loader:\n",
    "#         input_ids, attention_mask, labels = [x.to(\"cpu\") for x in batch]\n",
    "#         outputs = qat_quantized_model(input_ids, attention_mask=attention_mask)\n",
    "#         preds = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "#         labels = labels.cpu().tolist()\n",
    "#         all_preds.extend(preds)\n",
    "#         all_labels.extend(labels)\n",
    "# qat_accuracy = accuracy_score(all_labels, all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf46d9-7e06-4286-a63c-20532581c418",
   "metadata": {},
   "source": [
    "### Step 10: Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0209436f-2dbf-4c09-a08d-3d518b4246fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models_info = [\n",
    "    {\n",
    "        'Model': 'Original',\n",
    "        'Size (MB)': original_size,\n",
    "        'Accuracy': original_accuracy,\n",
    "        'Precision': original_precision,\n",
    "        'Recall': original_recall,\n",
    "        'F1-Score': original_f1,\n",
    "        'Latency (ms)': original_latency\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Dynamic PTQ',\n",
    "        'Size (MB)': dynamic_size,\n",
    "        'Accuracy': dynamic_accuracy,\n",
    "        'Precision': dynamic_precision,\n",
    "        'Recall': dynamic_recall,\n",
    "        'F1-Score': dynamic_f1,\n",
    "        'Latency (ms)': dynamic_latency\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Static PTQ',\n",
    "        'Size (MB)': static_size,\n",
    "        'Accuracy': static_accuracy,\n",
    "        'Precision': static_precision,\n",
    "        'Recall': static_recall,\n",
    "        'F1-Score': static_f1,\n",
    "        'Latency (ms)': static_latency\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Weight-Only PTQ',\n",
    "        'Size (MB)': weight_only_size,\n",
    "        'Accuracy': weight_only_accuracy,\n",
    "        'Precision': weight_only_precision,\n",
    "        'Recall': weight_only_recall,\n",
    "        'F1-Score': weight_only_f1,\n",
    "        'Latency (ms)': weight_only_latency\n",
    "    },\n",
    "    {\n",
    "        'Model': 'QAT',\n",
    "        'Size (MB)': qat_size,\n",
    "        'Accuracy': qat_accuracy,\n",
    "        'Precision': qat_precision,\n",
    "        'Recall': qat_recall,\n",
    "        'F1-Score': qat_f1,\n",
    "        'Latency (ms)': qat_latency\n",
    "    }\n",
    "]\n",
    "\n",
    "comparison_df = compare_models(models_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd14c7ea-6801-40b0-aaa2-d4ed72032075",
   "metadata": {},
   "source": [
    "### Save tokenizer for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7681c536-dda0-4d76-9293-e545da6ab943",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for model_alias in [original_model_alias, dynamic_ptq_model_alias, static_ptq_model_alias, weight_only_ptq_model_alias, qat_model_alias]:\n",
    "    tokenizer.save_pretrained(f\"model-metric/{model_alias}/tokenizer/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
