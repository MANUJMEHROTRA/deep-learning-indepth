{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece tensorboardX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft: 0.14.0\n",
      "Torch: 2.2.2\n",
      "Transformers: 4.49.0\n",
      "Accelerate: 1.4.0\n",
      "Huggingface Hub: 0.29.1\n"
     ]
    }
   ],
   "source": [
    "# --- Common Utilities and Setup ---\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "import accelerate\n",
    "import huggingface_hub\n",
    "import peft\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "import time\n",
    "import onnxruntime as ort\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import pickle\n",
    "\n",
    "print(\"peft:\", peft.__version__)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n",
    "print(\"Huggingface Hub:\", huggingface_hub.__version__)\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_dict(model_alias, MODEL_NAME):\n",
    "    if not os.path.exists('model_dict.json'):\n",
    "        model_dict = {}\n",
    "    else:\n",
    "        with open('model_dict.json', 'r') as file:\n",
    "            model_dict = json.load(file)\n",
    "\n",
    "    model_dict[model_alias] = MODEL_NAME\n",
    "\n",
    "    with open('model_dict.json', 'w') as file:\n",
    "        json.dump(model_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(filepath=\"./data/train-00000-of-00001-a5a7c6e4bb30b016.parquet\"):\n",
    "    \"\"\"Loads and preprocesses the dataset.\"\"\"\n",
    "    df = pd.read_parquet(filepath)\n",
    "    df = df[['conversation', 'issue_area']]\n",
    "    print(\"Original distribution:\\n\", df['issue_area'].value_counts())\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"labels\"] = label_encoder.fit_transform(df[\"issue_area\"])\n",
    "\n",
    "    #saving Label-encoder\n",
    "    label_encoder_path = f\"model-metric/{model_alias}/label_encoder.pkl\"\n",
    "    os.makedirs(os.path.dirname(label_encoder_path), exist_ok=True)\n",
    "    with open(label_encoder_path, \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "        \n",
    "    return df, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(df, max_count=100, random_state=42):\n",
    "    \"\"\"Balances the dataset using oversampling.\"\"\"\n",
    "    balanced_df = pd.DataFrame()\n",
    "    for issue in df['issue_area'].unique():\n",
    "        subset = df[df['issue_area'] == issue]\n",
    "        balanced_subset = resample(subset, replace=True, n_samples=max_count, random_state=random_state)\n",
    "        balanced_df = pd.concat([balanced_df, balanced_subset])\n",
    "    return balanced_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_conversation(conversation):\n",
    "    \"\"\"Preprocesses a conversation.\"\"\"\n",
    "    if isinstance(conversation, list):\n",
    "        return \" \".join([turn.get('text', '') for turn in conversation if isinstance(turn, dict)])\n",
    "    return str(conversation).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PyTorch Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=256):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        encoding = self.tokenizer(\n",
    "            row[\"conversation\"], padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        label = torch.tensor(row[\"labels\"], dtype=torch.long)\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(df, tokenizer, batch_size=8, train_ratio=0.75):\n",
    "    \"\"\"Creates train and test DataLoaders.\"\"\"\n",
    "    train_size = int(train_ratio * len(df))\n",
    "    train_df, test_df = df[:train_size], df[train_size:]\n",
    "    train_dataset = CustomDataset(train_df, tokenizer)\n",
    "    test_dataset = CustomDataset(test_df, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAMobileLLM(nn.Module):\n",
    "    def __init__(self, num_labels, lora_r=8, lora_alpha=16, lora_dropout=0.1):\n",
    "        super(LoRAMobileLLM, self).__init__()\n",
    "        # Load the base model with the correct number of labels\n",
    "        self.bert = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME\n",
    "            # num_labels=num_labels  # Ensure this matches the number of classes\n",
    "        )\n",
    "        \n",
    "        # LoRA Configuration\n",
    "        lora_config = LoraConfig(\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"]\n",
    "        )\n",
    "        self.model = get_peft_model(self.bert, lora_config)\n",
    "        \n",
    "        # Custom classifier for issue prediction\n",
    "        self.classifier = nn.Linear(self.model.config.hidden_size, num_labels)\n",
    "        self.classifier = nn.Linear(32000, num_labels)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get the model outputs\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # For CausalLM models, we need to use the last hidden state of the last token\n",
    "        # Get the last token for each sequence in the batch\n",
    "        batch_size = input_ids.shape[0]\n",
    "        last_token_indices = torch.sum(attention_mask, dim=1) - 1\n",
    "        \n",
    "        # Extract hidden states from the last layer\n",
    "        hidden_states = outputs.logits  # Shape: [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        # Use the representation at the first token (CLS token) for classification\n",
    "        cls_embedding = hidden_states[:, 0, :]\n",
    "        # print(\"Hidden state shape before classifier:\", cls_embedding.shape)\n",
    "        \n",
    "        # Pass through the classifier\n",
    "        return self.classifier(cls_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute class weights\n",
    "def compute_class_weights(labels, num_classes):\n",
    "    counter = Counter(labels)\n",
    "    total_samples = len(labels)\n",
    "    weights = [total_samples / (num_classes * counter[i]) for i in range(num_classes)]\n",
    "    return torch.tensor(weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, model_alias, epochs=3, learning_rate=5e-5, class_weights=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "\n",
    "    # Create directory for model metrics if it doesn't exist\n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # TensorBoard writer inside the model directory\n",
    "    writer = SummaryWriter(log_dir=model_dir)\n",
    "\n",
    "    # Convert class weights to float and move to device\n",
    "    if class_weights is not None:\n",
    "        class_weights = class_weights.float().to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Dictionary to store epoch-wise KPIs\n",
    "    kpi_results = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch_idx, (batch, label) in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            label = label.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "            \n",
    "            loss = criterion(logits, label)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Store predictions and labels for evaluation\n",
    "            preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            labels = label.cpu().tolist()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "            \n",
    "            # Log batch-level loss\n",
    "            if batch_idx % 10 == 0:\n",
    "                writer.add_scalar(\"BatchLoss/train\", loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "        # Compute epoch-level metrics\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "        epoch_time = time.time() - start_time  # Time taken per epoch\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Accuracy={accuracy:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1-score={f1:.4f}, Time={epoch_time:.2f}s\")\n",
    "\n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train\", accuracy, epoch)\n",
    "        writer.add_scalar(\"Precision/train\", precision, epoch)\n",
    "        writer.add_scalar(\"Recall/train\", recall, epoch)\n",
    "        writer.add_scalar(\"F1-score/train\", f1, epoch)\n",
    "        writer.add_scalar(\"EpochTime/train\", epoch_time, epoch)\n",
    "\n",
    "        # Store KPI results\n",
    "        kpi_results.append({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"loss\": avg_loss,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"time_taken\": epoch_time\n",
    "        })\n",
    "\n",
    "    # Save KPIs as a JSON file\n",
    "    import json\n",
    "    with open(f\"{model_dir}/training_metrics.json\", \"w\") as f:\n",
    "        json.dump(kpi_results, f, indent=4)\n",
    "\n",
    "    # Plot and save training loss curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, epochs+1), epoch_losses, marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Training Loss Over Epochs ({model_alias})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{model_dir}/training_loss.png\")\n",
    "    \n",
    "    # Add figure to TensorBoard\n",
    "    img = plt.gcf()\n",
    "    writer.add_figure(\"Training Loss\", img, close=True)\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, model_alias, label_encoder):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create directory for model evaluation if it doesn't exist\n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # TensorBoard writer inside the model directory\n",
    "    writer = SummaryWriter(log_dir=model_dir)\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, label in test_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            label = label.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(label.cpu().tolist())\n",
    "\n",
    "    # Get class names\n",
    "    class_names = label_encoder.classes_\n",
    "\n",
    "    # Compute per-class metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
    "    \n",
    "    # Create DataFrame for per-class metrics\n",
    "    class_metrics = pd.DataFrame({\n",
    "        'Class': class_names,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Support': support\n",
    "    })\n",
    "    \n",
    "    # Compute overall metrics\n",
    "    overall_precision, overall_recall, overall_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save confusion matrix\n",
    "    cm_path = os.path.join(model_dir, \"confusion_matrix.png\")\n",
    "    plt.savefig(cm_path)\n",
    "\n",
    "    # Log confusion matrix to TensorBoard\n",
    "    writer.add_figure(\"Confusion Matrix\", plt.gcf(), close=True)\n",
    "\n",
    "    # Display per-class metrics\n",
    "    print(\"\\nPer-class Metrics:\")\n",
    "    print(class_metrics.to_string(index=False))\n",
    "\n",
    "    print(f\"\\nOverall Metrics:\")\n",
    "    print(f\"Precision: {overall_precision:.4f}, Recall: {overall_recall:.4f}, F1-score: {overall_f1:.4f}\")\n",
    "\n",
    "    # Log overall metrics to TensorBoard\n",
    "    writer.add_scalar(\"Precision/test\", overall_precision)\n",
    "    writer.add_scalar(\"Recall/test\", overall_recall)\n",
    "    writer.add_scalar(\"F1-score/test\", overall_f1)\n",
    "\n",
    "    # Log per-class metrics to TensorBoard\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        writer.add_scalar(f\"Precision/{class_name}\", precision[i])\n",
    "        writer.add_scalar(f\"Recall/{class_name}\", recall[i])\n",
    "        writer.add_scalar(f\"F1-score/{class_name}\", f1[i])\n",
    "\n",
    "    # Save per-class metrics and confusion matrix as CSV\n",
    "    class_metrics.to_csv(os.path.join(model_dir, \"class_metrics.csv\"), index=False)\n",
    "    cm_df.to_csv(os.path.join(model_dir, \"confusion_matrix.csv\"))\n",
    "\n",
    "    # Save model checkpoint\n",
    "    model_path = os.path.join(model_dir, f\"{model_alias}.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "    return class_metrics, cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def export_to_onnx(model, tokenizer, onnx_path=\"model.onnx\"):\n",
    "#     \"\"\"Exports the model to ONNX format.\"\"\"\n",
    "#     model.eval().to(\"cpu\")\n",
    "#     sample_input = tokenizer(\"test\", return_tensors=\"pt\")\n",
    "#     input_names = [\"input_ids\", \"attention_mask\"]\n",
    "#     output_names = [\"output\"]\n",
    "#     torch.onnx.export(model, (sample_input[\"input_ids\"], sample_input[\"attention_mask\"]), onnx_path, input_names=input_names, output_names=output_names, dynamic_axes={\"input_ids\": {0: \"batch\", 1: \"sequence\"}, \"attention_mask\": {0: \"batch\", 1: \"sequence\"}, \"output\": {0: \"batch\"}})\n",
    "\n",
    "# def run_onnx_inference(onnx_path, input_ids, attention_mask):\n",
    "#     \"\"\"Runs inference using ONNX Runtime.\"\"\"\n",
    "#     ort_session = ort.InferenceSession(onnx_path)\n",
    "#     ort_inputs = {\"input_ids\": input_ids.tolist(), \"attention_mask\": attention_mask.tolist()}\n",
    "#     ort_outputs = ort_session.run(None, ort_inputs)\n",
    "#     return torch.tensor(np.array(ort_outputs[0]), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_onnx(model, tokenizer, model_alias):\n",
    "    \"\"\"Exports the MobileLLM-125M model to ONNX format.\"\"\"\n",
    "    model.eval().to(\"cpu\")\n",
    "    sample_input = tokenizer(\"test\", return_tensors=\"pt\")\n",
    "    input_names = [\"input_ids\", \"attention_mask\"]\n",
    "    output_names = [\"output\"]\n",
    "    \n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    onnx_path = os.path.join(model_dir, f\"{model_alias}.onnx\")\n",
    "    \n",
    "    torch.onnx.export(\n",
    "        model, \n",
    "        (sample_input[\"input_ids\"], sample_input[\"attention_mask\"]), \n",
    "        onnx_path, \n",
    "        input_names=input_names, \n",
    "        output_names=output_names, \n",
    "        dynamic_axes={\n",
    "            \"input_ids\": {0: \"batch\", 1: \"sequence\"}, \n",
    "            \"attention_mask\": {0: \"batch\", 1: \"sequence\"}, \n",
    "            \"output\": {0: \"batch\"}\n",
    "        }\n",
    "    )\n",
    "    print(f\"ONNX model exported to {onnx_path}\")\n",
    "    return onnx_path\n",
    "\n",
    "def run_onnx_inference(onnx_path, input_ids, attention_mask):\n",
    "    \"\"\"Runs inference using ONNX Runtime.\"\"\"\n",
    "    ort_session = ort.InferenceSession(onnx_path)\n",
    "    ort_inputs = {\"input_ids\": input_ids.tolist(), \"attention_mask\": attention_mask.tolist()}\n",
    "    ort_outputs = ort_session.run(None, ort_inputs)\n",
    "    return torch.tensor(np.array(ort_outputs[0]), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_inference_performance(model, tokenizer, test_df, label_encoder):\n",
    "    \"\"\"Compares inference performance between PyTorch and ONNX Runtime.\"\"\"\n",
    "    sample_batch = test_df.sample(50)\n",
    "    test_dataset_batch = CustomDataset(sample_batch, tokenizer)\n",
    "    test_loader_batch = DataLoader(test_dataset_batch, batch_size=len(sample_batch), shuffle=False)\n",
    "    batch = next(iter(test_loader_batch))\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    \n",
    "    # PyTorch Inference\n",
    "    model.eval().to(device)\n",
    "    start_time_torch = time.time()\n",
    "    with torch.no_grad():\n",
    "        torch_outputs = model(input_ids.to(device), attention_mask.to(device)).cpu()\n",
    "    end_time_torch = time.time()\n",
    "    latency_torch = end_time_torch - start_time_torch\n",
    "    throughput_torch = len(sample_batch) / latency_torch\n",
    "\n",
    "    # ONNX Inference\n",
    "    export_to_onnx(model, tokenizer)\n",
    "    start_time_onnx = time.time()\n",
    "    onnx_outputs = run_onnx_inference(\"model.onnx\", input_ids, attention_mask)\n",
    "    end_time_onnx = time.time()\n",
    "    latency_onnx = end_time_onnx - start_time_onnx\n",
    "    throughput_onnx = len(sample_batch) / latency_onnx\n",
    "\n",
    "    print(f\"PyTorch Inference - Latency: {latency_torch:.4f}s, Throughput: {throughput_torch:.2f} samples/s\")\n",
    "    print(f\"ONNX Inference - Latency: {latency_onnx:.4f}s, Throughput: {throughput_onnx:.2f} samples/s\")\n",
    "\n",
    "    torch_preds = torch.argmax(torch_outputs, dim=1).tolist()\n",
    "    onnx_preds = torch.argmax(onnx_outputs, dim=1).tolist()\n",
    "    actual_labels = labels.tolist()\n",
    "    \n",
    "    #compare predictions\n",
    "    print(\"Torch predictions vs Actual labels\")\n",
    "    print(classification_report(actual_labels, torch_preds, target_names = label_encoder.classes_))\n",
    "    print(\"Onnx predictions vs Actual labels\")\n",
    "    print(classification_report(actual_labels, onnx_preds, target_names = label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_inference_performance(model, tokenizer, test_df, label_encoder, model_alias):\n",
    "    \"\"\"Compares inference performance between PyTorch and ONNX Runtime.\"\"\"\n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir=model_dir)\n",
    "    \n",
    "    sample_batch = test_df.sample(50)\n",
    "    test_dataset_batch = CustomDataset(sample_batch, tokenizer)\n",
    "    test_loader_batch = DataLoader(test_dataset_batch, batch_size=len(sample_batch), shuffle=False)\n",
    "    batch = next(iter(test_loader_batch))\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    \n",
    "    # PyTorch Inference\n",
    "    model.eval().to(device)\n",
    "    start_time_torch = time.time()\n",
    "    with torch.no_grad():\n",
    "        torch_outputs = model(input_ids.to(device), attention_mask.to(device)).cpu()\n",
    "    latency_torch = time.time() - start_time_torch\n",
    "    throughput_torch = len(sample_batch) / latency_torch\n",
    "    \n",
    "    # ONNX Inference\n",
    "    onnx_path = export_to_onnx(model, tokenizer, model_alias)\n",
    "    start_time_onnx = time.time()\n",
    "    onnx_outputs = run_onnx_inference(onnx_path, input_ids, attention_mask)\n",
    "    latency_onnx = time.time() - start_time_onnx\n",
    "    throughput_onnx = len(sample_batch) / latency_onnx\n",
    "    \n",
    "    print(f\"PyTorch Inference - Latency: {latency_torch:.4f}s, Throughput: {throughput_torch:.2f} samples/s\")\n",
    "    print(f\"ONNX Inference - Latency: {latency_onnx:.4f}s, Throughput: {throughput_onnx:.2f} samples/s\")\n",
    "    \n",
    "    torch_preds = torch.argmax(torch_outputs, dim=1).tolist()\n",
    "    onnx_preds = torch.argmax(onnx_outputs, dim=1).tolist()\n",
    "    actual_labels = labels.tolist()\n",
    "    \n",
    "    # Compare predictions\n",
    "    torch_report = classification_report(actual_labels, torch_preds, target_names=label_encoder.classes_, output_dict=True)\n",
    "    onnx_report = classification_report(actual_labels, onnx_preds, target_names=label_encoder.classes_, output_dict=True)\n",
    "    \n",
    "    torch_df = pd.DataFrame(torch_report).transpose()\n",
    "    onnx_df = pd.DataFrame(onnx_report).transpose()\n",
    "    \n",
    "    torch_df.to_csv(os.path.join(model_dir, \"torch_classification_report.csv\"))\n",
    "    onnx_df.to_csv(os.path.join(model_dir, \"onnx_classification_report.csv\"))\n",
    "    \n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar(\"Latency/PyTorch\", latency_torch)\n",
    "    writer.add_scalar(\"Throughput/PyTorch\", throughput_torch)\n",
    "    writer.add_scalar(\"Latency/ONNX\", latency_onnx)\n",
    "    writer.add_scalar(\"Throughput/ONNX\", throughput_onnx)\n",
    "    \n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    return torch_df, onnx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def export_to_onnx(model, tokenizer, model_alias):\n",
    "    \"\"\"Exports MobileLLM-125M model to ONNX format.\"\"\"\n",
    "    model.eval().to(\"cpu\")\n",
    "    sample_input = tokenizer(\"test\", return_tensors=\"pt\")\n",
    "    input_names = [\"input_ids\", \"attention_mask\"]\n",
    "    output_names = [\"output\"]\n",
    "    \n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    onnx_path = os.path.join(model_dir, f\"{model_alias}.onnx\")\n",
    "    \n",
    "    torch.onnx.export(\n",
    "        model, \n",
    "        (sample_input[\"input_ids\"], sample_input[\"attention_mask\"]), \n",
    "        onnx_path, \n",
    "        input_names=input_names, \n",
    "        output_names=output_names, \n",
    "        dynamic_axes={\n",
    "            \"input_ids\": {0: \"batch\", 1: \"sequence\"}, \n",
    "            \"attention_mask\": {0: \"batch\", 1: \"sequence\"}, \n",
    "            \"output\": {0: \"batch\"}\n",
    "        }\n",
    "    )\n",
    "    print(f\"ONNX model exported to {onnx_path}\")\n",
    "    return onnx_path\n",
    "\n",
    "def run_onnx_inference(onnx_path, batch):\n",
    "    \"\"\"Runs inference using ONNX Runtime.\"\"\"\n",
    "    ort_session = ort.InferenceSession(onnx_path)\n",
    "    ort_inputs = {\"input_ids\": batch[\"input_ids\"].tolist(), \"attention_mask\": batch[\"attention_mask\"].tolist()}\n",
    "    ort_outputs = ort_session.run(None, ort_inputs)\n",
    "    return torch.tensor(np.array(ort_outputs[0]), dtype=torch.float32)\n",
    "\n",
    "def compare_inference_performance(model, tokenizer, test_df, label_encoder, model_alias):\n",
    "    \"\"\"Compares inference performance between PyTorch and ONNX Runtime.\"\"\"\n",
    "    model_dir = f\"model-metric/{model_alias}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir=model_dir)\n",
    "    \n",
    "    sample_batch = test_df.sample(50)\n",
    "    test_dataset_batch = CustomDataset(sample_batch, tokenizer)\n",
    "    test_loader_batch = DataLoader(test_dataset_batch, batch_size=len(sample_batch), shuffle=False)\n",
    "    batch, labels = next(iter(test_loader_batch))\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval().to(device)\n",
    "    \n",
    "    # PyTorch Inference\n",
    "    start_time_torch = time.time()\n",
    "    with torch.no_grad():\n",
    "        torch_outputs = model(input_ids=batch[\"input_ids\"].to(device), attention_mask=batch[\"attention_mask\"].to(device)).cpu()\n",
    "    latency_torch = time.time() - start_time_torch\n",
    "    throughput_torch = len(sample_batch) / latency_torch\n",
    "    \n",
    "    # ONNX Inference\n",
    "    onnx_path = export_to_onnx(model, tokenizer, model_alias)\n",
    "    start_time_onnx = time.time()\n",
    "    onnx_outputs = run_onnx_inference(onnx_path, batch)\n",
    "    latency_onnx = time.time() - start_time_onnx\n",
    "    throughput_onnx = len(sample_batch) / latency_onnx\n",
    "    \n",
    "    print(f\"PyTorch Inference - Latency: {latency_torch:.4f}s, Throughput: {throughput_torch:.2f} samples/s\")\n",
    "    print(f\"ONNX Inference - Latency: {latency_onnx:.4f}s, Throughput: {throughput_onnx:.2f} samples/s\")\n",
    "    \n",
    "    torch_preds = torch.argmax(torch_outputs, dim=1).tolist()\n",
    "    onnx_preds = torch.argmax(onnx_outputs, dim=1).tolist()\n",
    "    actual_labels = labels.tolist()\n",
    "    \n",
    "    # Classification Reports\n",
    "    torch_report = classification_report(actual_labels, torch_preds, target_names=label_encoder.classes_, output_dict=True)\n",
    "    onnx_report = classification_report(actual_labels, onnx_preds, target_names=label_encoder.classes_, output_dict=True)\n",
    "    \n",
    "    torch_df = pd.DataFrame(torch_report).transpose()\n",
    "    onnx_df = pd.DataFrame(onnx_report).transpose()\n",
    "    \n",
    "    torch_df.to_csv(os.path.join(model_dir, \"torch_classification_report.csv\"))\n",
    "    onnx_df.to_csv(os.path.join(model_dir, \"onnx_classification_report.csv\"))\n",
    "    \n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar(\"Latency/PyTorch\", latency_torch)\n",
    "    writer.add_scalar(\"Throughput/PyTorch\", throughput_torch)\n",
    "    writer.add_scalar(\"Latency/ONNX\", latency_onnx)\n",
    "    writer.add_scalar(\"Throughput/ONNX\", throughput_onnx)\n",
    "    \n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    return torch_df, onnx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_model_dict(model_alias, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution:\n",
      " issue_area\n",
      "Cancellations and returns    286\n",
      "Order                        270\n",
      "Login and Account            151\n",
      "Shopping                     116\n",
      "Warranty                     105\n",
      "Shipping                      72\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df, label_encoder = load_and_preprocess_data()\n",
    "balanced_df = balance_dataset(df)\n",
    "balanced_df['conversation'] = balanced_df['conversation'].apply(preprocess_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer for facebook/MobileLLM-125M\n",
    "MODEL_NAME = \"facebook/MobileLLM-125M\"\n",
    "model_alias = 'mobileLLM-125M'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "train_loader, test_loader, test_df = create_dataloaders(balanced_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for facebook/MobileLLM-125M contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/facebook/MobileLLM-125M.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/MobileLLM-125M were not used when initializing MobileLLMForCausalLM: {'lm_head.weight'}\n",
      "- This IS expected if you are initializing MobileLLMForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MobileLLMForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weights of each class is: tensor([1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Model Initialization and Training\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = LoRAMobileLLM(num_labels=num_classes)\n",
    "class_weights = compute_class_weights(balanced_df['labels'], num_classes)\n",
    "\n",
    "print(f\"class weights of each class is: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=3.0575, Accuracy=0.1467, Precision=0.1447, Recall=0.1467, F1-score=0.1447, Time=145.54s\n",
      "Epoch 2: Loss=2.1663, Accuracy=0.1578, Precision=0.1617, Recall=0.1578, F1-score=0.1575, Time=133.03s\n",
      "Epoch 3: Loss=2.0549, Accuracy=0.1933, Precision=0.1870, Recall=0.1933, F1-score=0.1888, Time=124.46s\n",
      "Epoch 4: Loss=2.0607, Accuracy=0.1556, Precision=0.1549, Recall=0.1556, F1-score=0.1535, Time=123.62s\n",
      "Epoch 5: Loss=1.9634, Accuracy=0.1800, Precision=0.1691, Recall=0.1800, F1-score=0.1689, Time=1035.03s\n",
      "Epoch 6: Loss=1.9844, Accuracy=0.1711, Precision=0.1750, Recall=0.1711, F1-score=0.1698, Time=570.61s\n",
      "Epoch 7: Loss=2.0650, Accuracy=0.1733, Precision=0.1733, Recall=0.1733, F1-score=0.1654, Time=109.12s\n",
      "Epoch 8: Loss=2.0550, Accuracy=0.1578, Precision=0.1500, Recall=0.1578, F1-score=0.1510, Time=110.38s\n",
      "Epoch 9: Loss=1.9728, Accuracy=0.2244, Precision=0.2158, Recall=0.2244, F1-score=0.2076, Time=122.09s\n",
      "Epoch 10: Loss=2.0942, Accuracy=0.1711, Precision=0.1664, Recall=0.1711, F1-score=0.1634, Time=117.81s\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, model_alias='mobileLLM-125M', epochs=10, learning_rate=5e-5, class_weights=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manmehro1/repository/fastapi-crud-app/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/manmehro1/repository/fastapi-crud-app/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/manmehro1/repository/fastapi-crud-app/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/manmehro1/repository/fastapi-crud-app/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/manmehro1/repository/fastapi-crud-app/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "Cancellations and returns       0.13      1.00      0.24        20\n",
      "        Login and Account       0.00      0.00      0.00        28\n",
      "                    Order       0.00      0.00      0.00        30\n",
      "                 Shipping       0.00      0.00      0.00        23\n",
      "                 Shopping       0.00      0.00      0.00        22\n",
      "                 Warranty       0.00      0.00      0.00        27\n",
      "\n",
      "                 accuracy                           0.13       150\n",
      "                macro avg       0.02      0.17      0.04       150\n",
      "             weighted avg       0.02      0.13      0.03       150\n",
      "\n",
      "\n",
      "Per-class Metrics:\n",
      "                    Class  Precision  Recall  F1-Score  Support\n",
      "Cancellations and returns   0.133333     1.0  0.235294       20\n",
      "        Login and Account   0.000000     0.0  0.000000       28\n",
      "                    Order   0.000000     0.0  0.000000       30\n",
      "                 Shipping   0.000000     0.0  0.000000       23\n",
      "                 Shopping   0.000000     0.0  0.000000       22\n",
      "                 Warranty   0.000000     0.0  0.000000       27\n",
      "\n",
      "Overall Metrics:\n",
      "Precision: 0.0178, Recall: 0.1333, F1-score: 0.0314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                       Class  Precision  Recall  F1-Score  Support\n",
       " 0  Cancellations and returns   0.133333     1.0  0.235294       20\n",
       " 1          Login and Account   0.000000     0.0  0.000000       28\n",
       " 2                      Order   0.000000     0.0  0.000000       30\n",
       " 3                   Shipping   0.000000     0.0  0.000000       23\n",
       " 4                   Shopping   0.000000     0.0  0.000000       22\n",
       " 5                   Warranty   0.000000     0.0  0.000000       27,\n",
       "                            Cancellations and returns  Login and Account  \\\n",
       " Cancellations and returns                         20                  0   \n",
       " Login and Account                                 28                  0   \n",
       " Order                                             30                  0   \n",
       " Shipping                                          23                  0   \n",
       " Shopping                                          22                  0   \n",
       " Warranty                                          27                  0   \n",
       " \n",
       "                            Order  Shipping  Shopping  Warranty  \n",
       " Cancellations and returns      0         0         0         0  \n",
       " Login and Account              0         0         0         0  \n",
       " Order                          0         0         0         0  \n",
       " Shipping                       0         0         0         0  \n",
       " Shopping                       0         0         0         0  \n",
       " Warranty                       0         0         0         0  )"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "evaluate_model(model, test_loader, model_alias, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model-metric/mobileLLM-125M/tokenizer/tokenizer_config.json',\n",
       " 'model-metric/mobileLLM-125M/tokenizer/special_tokens_map.json',\n",
       " 'model-metric/mobileLLM-125M/tokenizer/tokenizer.model',\n",
       " 'model-metric/mobileLLM-125M/tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(f\"model-metric/{model_alias}/tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manmehro1/.cache/huggingface/modules/transformers_modules/facebook/MobileLLM-125M/e09efb976ae4a767cf27a4bf277b7aade91dbd3a/modeling_mobilellm.py:1101: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model exported to model-metric/mobileLLM-125M/mobileLLM-125M.onnx\n",
      "PyTorch Inference - Latency: 6.3648s, Throughput: 7.86 samples/s\n",
      "ONNX Inference - Latency: 9.3653s, Throughput: 5.34 samples/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manmehro1/repository/fastapi-crud-app/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/manmehro1/repository/fastapi-crud-app/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/manmehro1/repository/fastapi-crud-app/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/manmehro1/repository/fastapi-crud-app/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/manmehro1/repository/fastapi-crud-app/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/manmehro1/repository/fastapi-crud-app/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                           precision    recall  f1-score  support\n",
       " Cancellations and returns     0.0000  0.000000  0.000000     7.00\n",
       " Login and Account             0.0000  0.000000  0.000000     9.00\n",
       " Order                         0.0000  0.000000  0.000000    10.00\n",
       " Shipping                      0.0000  0.000000  0.000000     7.00\n",
       " Shopping                      0.1800  1.000000  0.305085     9.00\n",
       " Warranty                      0.0000  0.000000  0.000000     8.00\n",
       " accuracy                      0.1800  0.180000  0.180000     0.18\n",
       " macro avg                     0.0300  0.166667  0.050847    50.00\n",
       " weighted avg                  0.0324  0.180000  0.054915    50.00,\n",
       "                            precision    recall  f1-score  support\n",
       " Cancellations and returns     0.0000  0.000000  0.000000     7.00\n",
       " Login and Account             0.0000  0.000000  0.000000     9.00\n",
       " Order                         0.0000  0.000000  0.000000    10.00\n",
       " Shipping                      0.0000  0.000000  0.000000     7.00\n",
       " Shopping                      0.1800  1.000000  0.305085     9.00\n",
       " Warranty                      0.0000  0.000000  0.000000     8.00\n",
       " accuracy                      0.1800  0.180000  0.180000     0.18\n",
       " macro avg                     0.0300  0.166667  0.050847    50.00\n",
       " weighted avg                  0.0324  0.180000  0.054915    50.00)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_inference_performance(model, tokenizer, test_df, label_encoder, model_alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "torch.save(model.state_dict(), \"./lora_distilbert_trained.pth\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Test with a sample input\n",
    "sample_text = \"Agent: Thank you for calling BrownBox Customer Support. My name is Tom. How may I assist you today?\"\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Move input to same device as model\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Perform inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get predicted label\n",
    "predicted_label = outputs.argmax().item()\n",
    "print(\"Predicted issue area:\", label_encoder.inverse_transform([predicted_label])[0])\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imporove model\n",
    "# tensorboard\n",
    "# training time vs gpu/cpu\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# try to use accelearte"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
