The **F1-score** and **F0.5-score** are both variations of the F-beta score, which is a metric used in classification problems to balance precision and recall. The difference lies in how they weigh precision and recall:

### **1. F1-score** (Î² = 1)
- **Formula**:  
  \[
  F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
  \]
- The F1-score gives **equal** importance to **precision** and **recall**.
- It is useful when **both false positives and false negatives are equally important**.

### **2. F0.5-score** (Î² = 0.5)
- **Formula**:  
  \[
  F_{0.5} = (1 + 0.5^2) \times \frac{\text{Precision} \times \text{Recall}}{(0.5^2 \times \text{Precision}) + \text{Recall}}
  \]
- The F0.5-score **weights precision more than recall**.
- It is useful when **false positives are more costly than false negatives** (i.e., precision is more important than recall).
- Example: In **spam detection**, predicting non-spam as spam (false positive) may be worse than missing a spam email (false negative).

### **Comparison Summary**
| Metric  | Weighting | When to Use |
|---------|----------|-------------|
| **F1-score**  | Equal weight to precision & recall | Balanced importance of false positives & false negatives |
| **F0.5-score** | Higher weight on precision | When false positives are costlier (e.g., spam detection, fraud detection) |

Would you like a practical example using Python? ðŸš€